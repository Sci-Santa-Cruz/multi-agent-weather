{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "289a4326-34f1-469e-87fc-4f70a7a11970",
   "metadata": {},
   "source": [
    "# Evaluaci√≥n del Agente\n",
    "\n",
    "## Objetivo\n",
    "Este notebook establece un marco sistem√°tico para evaluar el rendimiento y la eficacia de nuestro agente. Definiremos m√©tricas, crearemos conjuntos de prueba y realizaremos evaluaciones para medir objetivamente la calidad de las respuestas y decisiones del agente.\n",
    "\n",
    "En este notebook:\n",
    "- Test t√©cnicos \n",
    "- Definiremos m√©tricas cuantitativas y cualitativas\n",
    "- Construiremos conjuntos de prueba representativos\n",
    "- Implementaremos metodolog√≠as de evaluaci√≥n sistem√°tica\n",
    "- Analizaremos resultados para identificar fortalezas y debilidades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51a75e0-7720-4024-b4cb-0ff5082952c1",
   "metadata": {},
   "source": [
    "# Pruebas \n",
    "\n",
    "El sistema ser√° evaluado en funci√≥n de su precisi√≥n, rendimiento, capacidad para manejar errores, trazabilidad de ejecuciones, y la calidad de la interacci√≥n con los usuarios. Se llevar√°n a cabo pruebas t√©cnicas, cognitivas y √©ticas para asegurar que cumpla con los requisitos de funcionalidad, eficiencia y seguridad. A continuaci√≥n, se detallan los aspectos clave a medir en el proceso de evaluaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790c213d-bedc-4dfd-9bd9-5a36dad39914",
   "metadata": {},
   "source": [
    "## üß™ Pruebas T√©cnicas\n",
    "\n",
    "### 1. Pruebas de Robustez del Agente\n",
    "Estas pruebas validan qu√© tan bien el agente maneja entradas inesperadas, errores y condiciones adversas:\n",
    "\n",
    "- **Entrada malformada o incompleta**: El agente debe manejar consultas como `clma en Madird` o `divisas` sin pares definidos, sugiriendo correcci√≥n o solicitando aclaraci√≥n.\n",
    "  - _Ejemplo_: Si recibe `divisas`, deber√≠a responder: \"¬øPodr√≠as especificar qu√© par de divisas deseas consultar? (Ej: USD/MXN)\".\n",
    "- **Pruebas de latencia**: Mide el tiempo de respuesta de cada componente (API externa y LLM).\n",
    "  - _Herramientas_: `httpx`, `time.perf_counter()`.\n",
    "- **Fallas de red simuladas**: Simula errores como timeouts o respuestas 500.\n",
    "  - _Herramientas_: `responses`, `pytest-httpx`.\n",
    "\n",
    "### 2. Pruebas de Rendimiento\n",
    "Eval√∫an cu√°ntas peticiones por segundo puede manejar el agente sin degradaci√≥n:\n",
    "\n",
    "- **Carga concurrente**: Simular m√∫ltiples usuarios accediendo simult√°neamente.\n",
    "  - _Herramientas_: `locust`, `artillery`, `asyncio` con `aiohttp`.\n",
    "- **Pruebas de latencia**: Mide el tiempo de respuesta en condiciones normales y de carga.\n",
    "  - _Herramientas_: `httpx`, `time.perf_counter()`.\n",
    "\n",
    "### 3. Pruebas de Calidad y Seguridad del C√≥digo\n",
    "Validan la estabilidad, seguridad y mantenibilidad del c√≥digo base:\n",
    "\n",
    "- **Cobertura de c√≥digo**: Validar que los tests cubran rutas cr√≠ticas del c√≥digo.\n",
    "  - _Herramientas_: `pytest-cov`.\n",
    "- **Pruebas unitarias**: Validar funciones espec√≠ficas.\n",
    "  - _Herramientas_: `pytest`.\n",
    "- **Verificaci√≥n de tipos**: Validar que los tipos est√©n correctos.\n",
    "  - _Herramientas_: `mypy`.\n",
    "- **Seguridad de dependencias**: Verificar bibliotecas vulnerables.\n",
    "  - _Herramientas_: `safety`, `bandit`, `pip-audit`.\n",
    "- **Formato de c√≥digo**: Asegurar estilo consistente.\n",
    "  - _Herramientas_: `black`, `ruff`, `autopep8`.\n",
    "\n",
    "### 4. Control de Dependencias\n",
    "\n",
    "- Fijar versiones de dependencias para garantizar reproducibilidad.\n",
    "  - _Archivos_: `requirements.txt`, `poetry.lock`.\n",
    "- Escanear librer√≠as por vulnerabilidades conocidas.\n",
    "  - _Herramientas_: `pip-audit`, `safety`, `bandit`.\n",
    "\n",
    "### 5. Validaci√≥n de Formatos de Respuesta\n",
    "\n",
    "- Validar que cada servicio (clima, divisas, noticias) devuelva campos requeridos.\n",
    "  - _Herramientas_: `pydantic`, `cerberus`.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Pruebas Cognitivas\n",
    "\n",
    "Estas pruebas verifican que las respuestas del agente sean √∫tiles, claras y contextualmente relevantes.\n",
    "\n",
    "### 1. Desambiguaci√≥n y Contexto\n",
    "\n",
    "- Validar que el agente entienda solicitudes m√∫ltiples o ambiguas.\n",
    "  - _Ejemplo_: \"¬øQuiero saber el clima y las noticias?\" ‚Üí Responder ambos elementos.\n",
    "- Evaluar si el contexto de preguntas anteriores se mantiene.\n",
    "\n",
    "### 2. Consistencia\n",
    "\n",
    "- Evaluar si diferentes formulaciones de la misma pregunta generan respuestas coherentes.\n",
    "  - _Ejemplo_: \"¬øVa a llover en Oaxaca?\" vs \"¬øC√≥mo est√° el clima en Oaxaca?\"\n",
    "\n",
    "### 3. Lenguaje y Tono\n",
    "\n",
    "- Verificar que use un lenguaje comprensible y apropiado para el usuario.\n",
    "- Mantener consistencia en idioma y tono conversacional.\n",
    "\n",
    "### 4. Evaluaci√≥n Automatizada\n",
    "\n",
    "- Evaluar precisi√≥n y fundamentaci√≥n de respuestas.\n",
    "  - _Herramientas_: `LangSmith`, `Ragas`, `TruLens`.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öñÔ∏è Pruebas √âticas\n",
    "\n",
    "Estas pruebas buscan minimizar sesgos, errores y respuestas inapropiadas del agente.\n",
    "\n",
    "### 1. Neutralidad\n",
    "\n",
    "- Validar que el agente no se base en una sola fuente sesgada para noticias.\n",
    "  - _Ejemplo_: \"Fuente A opina X, pero Fuente B menciona Y\".\n",
    "\n",
    "### 2. Desinformaci√≥n y Alucinaciones\n",
    "\n",
    "- Verificar que el agente reconozca cuando no tiene informaci√≥n.\n",
    "  - _Ejemplo_: \"No tengo suficiente informaci√≥n para darte una respuesta certera.\"\n",
    "\n",
    "---\n",
    "\n",
    "## üõ°Ô∏è Pruebas de Seguridad\n",
    "\n",
    "### 1. Control de Permisos\n",
    "\n",
    "- Verificar que el agente tenga solo acceso m√≠nimo necesario a recursos.\n",
    "  - _Ejemplo_: API keys de solo lectura.\n",
    "\n",
    "### 3. Defensa en Profundidad\n",
    "\n",
    "- Uso combinado de sandboxing, validaci√≥n de entradas y reducci√≥n de privilegios.\n",
    "  - _Ejemplo_: acceso a archivos limitado a un directorio espec√≠fico.\n",
    "\n",
    "### 4. Casos Espec√≠ficos\n",
    "\n",
    "- Lectura y escritura de archivos fuera de scope.\n",
    "- Modificaci√≥n de bases de datos.\n",
    "  - _Medidas_: Regex en nombres, credenciales limitadas, entornos aislados.\n",
    "\n",
    "### 5. Protecci√≥n Contra Abuso\n",
    "\n",
    "- Verificaci√≥n de cuenta (email/tel√©fono).\n",
    "- Rate limiting por IP o usuario.\n",
    "  - _Herramientas_: Middleware personalizado.\n",
    "- Auditor√≠a de uso del LLM.\n",
    "\n",
    "### 6. Protecci√≥n Contra Prompt Injection y Abusos por Prompts Maliciosos\n",
    "\n",
    "- Dise√±ar prompts robustos y restringidos.\n",
    "- Validar salidas con herramientas externas.\n",
    "- Prevenir respuestas ofensivas, peligrosas o que puedan inducir al agente a actuar de forma inadecuada.\n",
    "  - _Herramientas_: `PromptLayer`, `guardrails`, `Rebuff`, pruebas manuales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5df1f59-899a-44bf-adfe-48bf98208eef",
   "metadata": {},
   "source": [
    "# Plan de Evaluaci√≥n para un Agente Especializado en Noticias, Clima y Divisas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc60f98-11bd-48fb-95c9-32bd206f70f0",
   "metadata": {},
   "source": [
    "\n",
    "### Evaluaci√≥n Ad Hoc\n",
    "\n",
    "Dado que este agente no busca resolver tareas abiertas o generalistas como en benchmarks tradicionales tipo **ARENA**, **MT-Bench** o **HELMeval**, **no se utilizar√°n frameworks cl√°sicos de evaluaci√≥n de LLMs**.  \n",
    "El agente se especializa en tareas **determin√≠sticas y orientadas a datos concretos**: entregar el **clima actual**, el **tipo de cambio de divisas**, y **res√∫menes o titulares de noticias**.\n",
    "\n",
    "Por ello, se justifica una **evaluaci√≥n ad hoc**, enfocada en **tres dimensiones** operativas cr√≠ticas:  \n",
    "- Correcta invocaci√≥n de herramientas,  \n",
    "- Calidad del razonamiento paso a paso, y  \n",
    "- Precisi√≥n de la respuesta final visible al usuario.\n",
    "\n",
    "---\n",
    "\n",
    "### Objetivo General\n",
    "\n",
    "Evaluar el comportamiento del agente en tres niveles complementarios:\n",
    "\n",
    "1. **Evaluaci√≥n End-to-End**  \n",
    "   Eval√∫a la respuesta completa generada por el agente ante una entrada del usuario. Se valida si entrega la informaci√≥n correcta, bien estructurada y con formato √∫til.\n",
    "\n",
    "2. **Evaluaci√≥n de Un Solo Paso (`One Step`)**  \n",
    "   Examina si el agente eligi√≥ la herramienta adecuada (por ejemplo, consulta del clima o divisas) y si la llamada incluye los par√°metros correctos.\n",
    "\n",
    "3. **Evaluaci√≥n de la Trayectoria (`Trajectory`)**  \n",
    "   Revisa la secuencia de nodos o herramientas utilizadas por el agente. Se analiza si los pasos seguidos tienen coherencia con la tarea solicitada y si hay desviaciones innecesarias o errores de flujo.\n",
    "\n",
    "---\n",
    "\n",
    "###  Herramientas y Evaluadores\n",
    "\n",
    "Se emplear√°n las siguientes herramientas para orquestar y aplicar las evaluaciones:\n",
    "\n",
    "- **LangSmith**: Plataforma para trazabilidad y an√°lisis de flujos, donde se visualizan las interacciones y trayectorias del agente, ayudando a verificar la secuencia de herramientas utilizadas y su consistencia con el flujo esperado.\n",
    "\n",
    "- **Evaluadores personalizados**, seg√∫n el tipo de prueba:\n",
    "  - **Heur√≠sticos**\n",
    "  - **LLM-as-a-judge**\n",
    "  - **Evaluadores humanos**\n",
    "\n",
    "---\n",
    "\n",
    "###  Dataset de Evaluaci√≥n\n",
    "\n",
    "Se construir√° un **dataset de pares pregunta-respuesta**, bajo el siguiente esquema:\n",
    "\n",
    "- **Entrada del usuario**: Cada entrada del dataset contiene una **pregunta/entrada** del usuario relacionada con el clima, las divisas o las noticias.\n",
    "- **Respuesta esperada**: Cada entrada tiene una **respuesta correcta esperada** seg√∫n la fuente de datos (por ejemplo, valores actualizados de divisas, condiciones meteorol√≥gicas actuales, titulares de noticias).\n",
    "- **Evaluaci√≥n de un paso**: Para evaluar un paso espec√≠fico del agente, se incluir√°n respuestas relacionadas con la **herramienta utilizada** y los par√°metros correctos. Esto permitir√° verificar si el agente hizo la invocaci√≥n correcta y con los valores adecuados.\n",
    "- **Evaluaci√≥n de la trayectoria**: Se incluir√°n listas espec√≠ficas de las **herramientas correctas** y el **flujo de nodos esperado** para comparar la secuencia de acciones del agente. Este paso valida si el agente sigue el flujo correcto sin desviaciones innecesarias.\n",
    "\n",
    "El dataset puede ser generado:\n",
    "- **Manual**, con ejemplos redactados por humanos.\n",
    "- **Sint√©tico**, con ayuda de scripts o plantillas para cubrir variabilidad y volumen.\n",
    "\n",
    "Este dataset se usar√° tanto para la evaluaci√≥n de outputs finales como para validaci√≥n intermedia de pasos o trayectorias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc76d5a-ca91-4888-b98c-5d05716a3af7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##  Evaluaci√≥n del Agente\n",
    "\n",
    "### 1. **Evaluaci√≥n de un Solo Paso**\n",
    "\n",
    "- **Inputs**: Entrada del usuario + herramientas.\n",
    "- **Output**: Respuesta generada por el LLM.\n",
    "- **Evaluador**: Puntuaci√≥n binaria (correcto/incorrecto) para la herramienta seleccionada y sus par√°metros.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Evaluaci√≥n de la Respuesta Final**\n",
    "\n",
    "- **Inputs**: Entrada del usuario + herramientas opcionales.\n",
    "- **Output**: Respuesta final del agente.\n",
    "- **Evaluador**: LLM-as-a-judge comparando con la respuesta esperada.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Evaluaci√≥n de la Trayectoria**\n",
    "\n",
    "- **Inputs**: Entrada del usuario + herramientas predefinidas.\n",
    "- **Output**: Secuencia de herramientas utilizadas.\n",
    "- **Evaluador**: Comparaci√≥n con la secuencia esperada de herramientas y puntuaci√≥n binaria o por errores.\n",
    "\n",
    "---\n",
    "\n",
    "### **Puntuaci√≥n y M√©tricas**\n",
    "\n",
    "- **Un Solo Paso**: Puntuaci√≥n binaria (correcto/incorrecto).\n",
    "- **Respuesta Final**: Puntuaci√≥n de 0 a 10 seg√∫n calidad.\n",
    "- **Trayectoria**: Puntuaci√≥n binaria o m√©trica de errores en la secuencia.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d20c76-c307-41db-8b88-02a789a50f4c",
   "metadata": {},
   "source": [
    "\n",
    "## **Plan de Evaluaci√≥n en Producci√≥n**\n",
    "\n",
    "De acuerdo con el enfoque y la tecnolog√≠a seleccionada para la productivizaci√≥n, se deben elegir herramientas adecuadas para trazabilidad y monitoreo. Independientemente de la soluci√≥n, los puntos clave a medir son los siguientes:\n",
    "\n",
    "### 1. **Rastreo de Ejecuciones**\n",
    "\n",
    "- **Configurar trazabilidad**: Activar el rastreo de ejecuciones del agente para capturar todas las interacciones.\n",
    "- **M√©tricas clave**: \n",
    "  - **Volumen de trazas**: N√∫mero de ejecuciones procesadas.\n",
    "  - **Tasa de √©xito/fracaso**: Evaluaci√≥n del desempe√±o del agente.\n",
    "  - **Latencia**: Tiempo de respuesta por ejecuci√≥n.\n",
    "  - **Conteo de tokens y costo**: Monitorear el consumo de recursos.\n",
    "\n",
    "### 2. **Recopilaci√≥n de Feedback**\n",
    "\n",
    "- **Feedback expl√≠cito**: Implementar botones de \"me gusta/no me gusta\" o formularios para recolectar satisfacci√≥n directa de los usuarios.\n",
    "- **Feedback impl√≠cito**: Monitorear patrones de uso como frecuencia de interacci√≥n o abandono.\n",
    "- **Evaluaci√≥n autom√°tica con LLM**: Usar evaluadores autom√°ticos para detectar respuestas err√≥neas, alucinaciones o respuestas t√≥xicas.\n",
    "\n",
    "### 3. **Monitoreo de Errores**\n",
    "\n",
    "- **Identificaci√≥n de errores**: Rastrear fallos y excepciones durante la ejecuci√≥n del agente.\n",
    "- **Ajustes**: Corregir los errores y realizar ajustes en tiempo real con base en los datos recolectados.\n",
    "\n",
    "### 4. **Clasificaci√≥n y Etiquetado**\n",
    "\n",
    "- **Clasificaci√≥n de respuestas**: Implementar un sistema para etiquetar las respuestas en funci√≥n de su relevancia, tono, y posibles problemas (toxicidad, etc.).\n",
    "- **Ajustes continuos**: Basar los ajustes en los resultados de la clasificaci√≥n y el feedback para mejorar el rendimiento del agente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639e5fd6-f6c2-4144-90d2-478d61c11c94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
