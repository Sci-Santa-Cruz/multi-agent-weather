{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08f76abc-3f8b-4882-bd10-5085028c352b",
   "metadata": {},
   "source": [
    "# Ingenier√≠a de Prompts  & Prototipado del Agente\n",
    "\n",
    "#### **Objetivo**\n",
    "Desarrollar y prototipar el agente mediante el dise√±o de prompts efectivos y su integraci√≥n en un prototipo funcional. Este notebook combina la ingenier√≠a de prompts con el prototipado del agente, permitiendo iteraciones y pruebas r√°pidas.\n",
    "\n",
    "#### **Secciones**\n",
    "\n",
    "1. **Ingenier√≠a de Prompts**\n",
    "   - Dise√±ar y probar diversas estructuras de prompts para asegurar que el modelo realice las tareas deseadas.\n",
    "   - Crear prompts espec√≠ficos para cada responsabilidad del agente.\n",
    "   - Evaluar la efectividad de los prompts a trav√©s de pruebas iniciales.\n",
    "\n",
    "2. **Prototipado del Agente**\n",
    "   - Implementar la estructura b√°sica del agente e integrar los prompts dise√±ados.\n",
    "   - Desarrollar funciones esenciales (ej., llamadas a APIs, manejo de entradas).\n",
    "   - Ejecutar pruebas iniciales y validar el rendimiento del agente.\n",
    "\n",
    "3. **Iteraci√≥n y Refinamiento**\n",
    "   - Ajustar los prompts y funciones del agente seg√∫n los resultados de las pruebas.\n",
    "   - Refinar el agente para mejorar su desempe√±o."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138948f9-a5e3-45e7-9109-be1aa5a559cc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Prototipado del Nodo `classify_tasks` y su Prompt\n",
    "\n",
    "En esta etapa se realiza el prototipado del primer componente del agente: el nodo `classify_tasks`. Este nodo es responsable de interpretar la entrada del usuario y clasificar qu√© tareas est√°n presentes (clima, divisas, noticias). Adem√°s del c√≥digo funcional, se prueba y valida el prompting espec√≠fico que permitir√° al modelo ejecutar esta clasificaci√≥n de forma consistente.\n",
    "\n",
    "---\n",
    "\n",
    "#### üéØ Objetivo de la prueba\n",
    "\n",
    "Validar el correcto funcionamiento del nodo `classify_tasks`, incluyendo:\n",
    "\n",
    "- El dise√±o y comportamiento del prompt que gu√≠a al LLM para identificar tareas dentro de un `user_input`.\n",
    "- El manejo de errores en la llamada al modelo.\n",
    "- La consistencia en la estructura de la respuesta.\n",
    "\n",
    "---\n",
    "\n",
    "#### üß† L√≥gica del Nodo\n",
    "\n",
    "`classify_tasks` debe:\n",
    "\n",
    "- Recibir una entrada de usuario (`user_input`).\n",
    "- Usar un prompt dise√±ado para pedir al LLM que identifique si el texto menciona temas de:\n",
    "  - Clima\n",
    "  - Tipo de cambio\n",
    "  - Noticias\n",
    "- Devolver un diccionario con:\n",
    "  ```python\n",
    "  {\n",
    "      \"tasks\": {\n",
    "          \"weather\": True/False,\n",
    "          \"currency\": True/False,\n",
    "          \"news\": True/False\n",
    "      },\n",
    "      \"error\": None o mensaje de error\n",
    "  }\n",
    "  ```\n",
    "\n",
    "En caso de error (por ejemplo, problemas con el LLM), el campo `\"error\"` debe incluir una descripci√≥n del fallo, y `\"tasks\"` tendr√° todos los valores en `False`.\n",
    "\n",
    "---\n",
    "\n",
    "#### Componentes de la prueba\n",
    "\n",
    "1. **Prompting**  \n",
    "   Validar que el prompt sea claro, espec√≠fico y conduzca al LLM a generar una salida estructurada y predecible. Un ejemplo de prompt podr√≠a ser:\n",
    "\n",
    "   ```\n",
    "   Dada la siguiente entrada del usuario, identifica si contiene menciones sobre:\n",
    "\n",
    "   - Clima\n",
    "   - Tipo de cambio (divisas)\n",
    "   - Noticias generales\n",
    "\n",
    "   Devuelve el resultado en formato JSON como este:\n",
    "   {\n",
    "     \"tasks_to_do\": {\n",
    "       \"weather\": true/false,\n",
    "       \"currency\": true/false,\n",
    "       \"news\": true/false\n",
    "     }\n",
    "   }\n",
    "\n",
    "   Entrada: <<user_input>>\n",
    "   ```\n",
    "\n",
    "2. **Pruebas funcionales del nodo**\n",
    "   - Entradas con menciones claras de cada tipo de tarea.\n",
    "   - Entradas mixtas (clima + noticias, divisas + clima, etc.).\n",
    "   - Entradas ambiguas o vac√≠as.\n",
    "   - Casos de error simulados (por ejemplo, lanzar una excepci√≥n desde el LLM).\n",
    "\n",
    "---\n",
    "\n",
    "#### Pasos para ejecutar la prueba\n",
    "\n",
    "1. **Configura el entorno**  \n",
    "   - Aseg√∫rate de tener instalada tu librer√≠a LLM (por ejemplo, `openai`) y tener una clave API v√°lida.\n",
    "   - Carga o define tu funci√≥n `classify_tasks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc99bdd6-0284-4fcc-821e-90acb7f4c32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ir al directorio principal\n",
    "from os import chdir\n",
    "\n",
    "chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a782be8-1f8e-4c78-a888-e55bcaf74022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar varibles\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='env')\n",
    "\n",
    "from utils.logging import setup_logging\n",
    "\n",
    "# Initialize logger using the setup_logging function\n",
    "logger = setup_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbcfe81-1d71-4495-a5d1-1f970dfc58ec",
   "metadata": {},
   "source": [
    "### Instala las bibliotecas necesarias\n",
    "\n",
    "Ejecuta el siguiente comando en la terminal:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28b831c7-d99b-442b-8e32-6488bad16728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ----- Modelo -----\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03611ebf-6369-4a54-a176-6f92c6ebaac6",
   "metadata": {},
   "source": [
    "### Definir el `state` que compartir√°n los nodos y agentes del grafo\n",
    "\n",
    "Este `state` es el contenedor central de informaci√≥n que se ir√° actualizando y transmitiendo entre nodos a lo largo del flujo definido en el grafo de LangGraph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1f3b1bb-ba3c-4c5d-8145-50483a6148a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile core/agent_state.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from typing import Annotated, TypedDict, Optional, List, Dict, Any\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv(dotenv_path='env')\n",
    "\n",
    "# Funci√≥n para combinar diccionarios\n",
    "def merge_dicts(dict1, dict2):\n",
    "    if not isinstance(dict1, dict) or not isinstance(dict2, dict):\n",
    "        raise TypeError(f\"Ambos argumentos deben ser diccionarios. Recibido: {type(dict1)} y {type(dict2)}\")\n",
    "    return {**dict1, **dict2}\n",
    "\n",
    "def add_history_update(history_old: List[str], history_new: List[str]) -> List[str]:\n",
    "    return history_old + history_new\n",
    "\n",
    "# Clase AgentState\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Representa el estado que fluye a trav√©s del agente LangGraph.\n",
    "\n",
    "    Atributos:\n",
    "        messages (list[BaseMessage]):\n",
    "            Lista de mensajes intercambiados entre el sistema y el usuario,\n",
    "            gestionados autom√°ticamente con el paso de mensajes de LangGraph.\n",
    "        \n",
    "        order_task (Optional[List[str]]):\n",
    "            Lista opcional que representa el orden en el que las tareas identificadas \n",
    "            deben ser procesadas o presentadas.\n",
    "\n",
    "        results (Dict[str, Any]):\n",
    "            Diccionario que contiene los resultados de varios nodos de tareas.\n",
    "            Cada clave es el nombre de la tarea, y el valor es el resultado o error.\n",
    "\n",
    "        error (Optional[str]):\n",
    "            Mensaje de error general para representar cualquier problema en el flujo del agente.\n",
    "\n",
    "        tasks_to_do (Dict[str, bool]):\n",
    "            Diccionario que representa qu√© tareas han sido identificadas para ejecutar.\n",
    "            Ejemplo: {\"weather\": True, \"currencies\": True, \"news\": False}\n",
    "\n",
    "        ready_to_aggregate (bool):\n",
    "            Indica si todas las tareas esperadas est√°n listas y el paso final de agregaci√≥n puede ejecutarse.\n",
    "\n",
    "        history (List[str]):\n",
    "            Lista para realizar un seguimiento de los nombres de los nodos por los que pasa el flujo.\n",
    "    \"\"\"\n",
    "    \n",
    "    messages: Annotated[List[BaseMessage], add_messages]  # Mensajes intercambiados\n",
    "    order_task: Dict[str, Any]  # Orden de las tareas\n",
    "    error: Annotated[Dict[str, str], merge_dicts]  # Manejo de errores\n",
    "    results: Annotated[Dict[str, str], merge_dicts]  # Resultados de las tareas\n",
    "    task_completed: Annotated[Dict[str, str], merge_dicts]  # Tareas completadas\n",
    "    tasks_to_do: Dict[str, bool]  # Tareas pendientes\n",
    "    ready_to_aggregate: bool  # Indicador de si est√° listo para agregarse\n",
    "    history: Annotated[List[str], add_history_update]  # Historial de nodos procesados\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0786bae1-e6a9-4eac-905b-aeb366aece07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile nodes/classify_query.py\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import logging\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import cast\n",
    "from core.agent_state import AgentState\n",
    "import json\n",
    "\n",
    "from utils.logging import setup_logging\n",
    "\n",
    "# Initialize logger using the setup_logging function\n",
    "logger = setup_logging()\n",
    "\n",
    "# ----- Cargar variables de entorno -----\n",
    "load_dotenv(dotenv_path='env')\n",
    "\n",
    "# Global LLM instance\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "# ----- System message (prompt) -----\n",
    "system_prompt = SystemMessage(\n",
    "    content=\"\"\"\n",
    "Eres un asistente que clasifica tareas en tres categor√≠as: weather, exchange y news.\n",
    "Dado un mensaje del usuario, responde con un JSON con claves: \"weather\", \"exchange\", \"news\",\n",
    "y valores booleanos indicando si la tarea est√° presente.\n",
    "Ejemplo de respuesta: {\"weather\": true, \"exchange\": false, \"news\": true}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# ----- Node: classify_tasks -----\n",
    "def classify_tasks(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Clasifica la intenci√≥n del usuario en categor√≠as predefinidas\n",
    "    usando un modelo de lenguaje y actualiza el estado.\n",
    "    \"\"\"\n",
    "    # A√±adir trazabilidad del nodo\n",
    "    state.setdefault(\"history\", []).append(\"classify_tasks\")\n",
    "    try:\n",
    "        logger.debug(\"Buscando el √∫ltimo mensaje del usuario...\")\n",
    "        user_msg = [m for m in state[\"messages\"] if isinstance(m, HumanMessage)][-1]\n",
    "        logger.info(f\"Mensaje recibido: {user_msg.content}\")\n",
    "\n",
    "        full_prompt = [system_prompt, user_msg]\n",
    "\n",
    "        logger.debug(\"Enviando prompt al modelo...\")\n",
    "        response = llm.invoke(full_prompt)\n",
    "        logger.debug(f\"Respuesta del modelo: {response.content}\")\n",
    "\n",
    "        classification = json.loads(response.content)\n",
    "        classification = cast(dict, classification)\n",
    "        logger.info(f\"Tareas clasificadas: {classification}\")\n",
    "\n",
    "        new_state = {\n",
    "            \"tasks_to_do\": classification,\n",
    "            \"results\": state.get(\"results\", {}),\n",
    "            \"task_completed\": {},\n",
    "            \"error\": {},\n",
    "            \"order_task\": {},\n",
    "            \"ready_to_aggregate\": False,\n",
    "        }\n",
    "\n",
    "        logger.debug(\"Estado actualizado correctamente.\")\n",
    "        return new_state\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"classify: {str(e)}\"\n",
    "        logger.exception(\"Error al clasificar el mensaje del usuario.\")\n",
    "        return {\n",
    "            \"messages\": state[\"messages\"] + [SystemMessage(content=error_msg)],\n",
    "            \"results\": state.get(\"results\", {}),\n",
    "            \"tasks_to_do\": {},\n",
    "            \"error\": {\"classify\": error_msg},\n",
    "            \"order_task\": {},\n",
    "            \"ready_to_aggregate\": False,\n",
    "            \"task_completed\": {}\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eb68aa-d0b2-4530-9fa8-94b7b98c2853",
   "metadata": {},
   "source": [
    "2. **Ejecuta ejemplos de prueba**\n",
    "\n",
    " Define diferentes entradas de usuario para probar varios escenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c08d96e2-44ae-4acf-98e4-b49a5e9ea005",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = [\n",
    "    # Casos con claridad\n",
    "    \"¬øC√≥mo est√° el clima en Ciudad de M√©xico?\",  # Clima\n",
    "    \"¬øCu√°l es el tipo de cambio de USD a EUR hoy?\",  # Divisas\n",
    "    \"Dame las √∫ltimas noticias de M√©xico.\",  # Noticias\n",
    "    # Casos combinados\n",
    "    \"Quiero saber el clima y las noticias.\",  # Clima, Noticias\n",
    "    \"Tipo de cambio del d√≥lar.\",  # Divisas\n",
    "    \"Noticias del f√∫tbol y el clima ma√±ana.\",  # Clima, Noticias\n",
    "    # Casos ambiguos o sin relaci√≥n\n",
    "    \"Algo completamente sin relaci√≥n.\",  # Ninguno\n",
    "    \"No entiendo qu√© pasa con el clima\",  # Clima (con ambig√ºedad)\n",
    "    \"El d√≥lar est√° cayendo, pero no s√© sobre el clima\",  # Divisas\n",
    "    \"¬øHay noticias del f√∫tbol?\",  # Noticias\n",
    "    # Casos de error\n",
    "    \"\",  # Entrada vac√≠a\n",
    "    \"    \",  # Entrada vac√≠a con espacios\n",
    "    \"¬øC√≥mo va el clima pero no s√© si te refieres a Ciudad de M√©xico?\"  # Clima, ambig√ºedad\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e0a403-b1cc-4067-a453-907b6a6b717a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8c759ca-8351-4840-af9b-f0887164d0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada: '¬øC√≥mo est√° el clima en Ciudad de M√©xico?'\n",
      "Salida: {'tasks_to_do': {'weather': True, 'exchange': False, 'news': False}, 'results': {}, 'task_completed': {}, 'error': {}, 'order_task': {}, 'ready_to_aggregate': False}\n",
      "----------------------------------------\n",
      "Entrada: '¬øCu√°l es el tipo de cambio de USD a EUR hoy?'\n",
      "Salida: {'tasks_to_do': {'weather': False, 'exchange': True, 'news': False}, 'results': {}, 'task_completed': {}, 'error': {}, 'order_task': {}, 'ready_to_aggregate': False}\n",
      "----------------------------------------\n",
      "Entrada: 'Dame las √∫ltimas noticias de M√©xico.'\n",
      "Salida: {'tasks_to_do': {'weather': False, 'exchange': False, 'news': True}, 'results': {}, 'task_completed': {}, 'error': {}, 'order_task': {}, 'ready_to_aggregate': False}\n",
      "----------------------------------------\n",
      "Entrada: 'Quiero saber el clima y las noticias.'\n",
      "Salida: {'tasks_to_do': {'weather': True, 'exchange': False, 'news': True}, 'results': {}, 'task_completed': {}, 'error': {}, 'order_task': {}, 'ready_to_aggregate': False}\n",
      "----------------------------------------\n",
      "Entrada: 'Tipo de cambio del d√≥lar.'\n",
      "Salida: {'tasks_to_do': {'weather': False, 'exchange': True, 'news': False}, 'results': {}, 'task_completed': {}, 'error': {}, 'order_task': {}, 'ready_to_aggregate': False}\n",
      "----------------------------------------\n",
      "Entrada: 'Noticias del f√∫tbol y el clima ma√±ana.'\n",
      "Salida: {'tasks_to_do': {'weather': True, 'exchange': False, 'news': True}, 'results': {}, 'task_completed': {}, 'error': {}, 'order_task': {}, 'ready_to_aggregate': False}\n",
      "----------------------------------------\n",
      "Entrada: 'Algo completamente sin relaci√≥n.'\n",
      "Salida: {'tasks_to_do': {'weather': False, 'exchange': False, 'news': False}, 'results': {}, 'task_completed': {}, 'error': {}, 'order_task': {}, 'ready_to_aggregate': False}\n",
      "----------------------------------------\n",
      "Entrada: 'No entiendo qu√© pasa con el clima'\n",
      "Salida: {'tasks_to_do': {'weather': True, 'exchange': False, 'news': False}, 'results': {}, 'task_completed': {}, 'error': {}, 'order_task': {}, 'ready_to_aggregate': False}\n",
      "----------------------------------------\n",
      "Entrada: 'El d√≥lar est√° cayendo, pero no s√© sobre el clima'\n",
      "Salida: {'tasks_to_do': {'weather': False, 'exchange': True, 'news': True}, 'results': {}, 'task_completed': {}, 'error': {}, 'order_task': {}, 'ready_to_aggregate': False}\n",
      "----------------------------------------\n",
      "Entrada: '¬øHay noticias del f√∫tbol?'\n",
      "Salida: {'tasks_to_do': {'weather': False, 'exchange': False, 'news': True}, 'results': {}, 'task_completed': {}, 'error': {}, 'order_task': {}, 'ready_to_aggregate': False}\n",
      "----------------------------------------\n",
      "Entrada: ''\n",
      "Salida: {'tasks_to_do': {'weather': False, 'exchange': False, 'news': False}, 'results': {}, 'task_completed': {}, 'error': {}, 'order_task': {}, 'ready_to_aggregate': False}\n",
      "----------------------------------------\n",
      "Entrada: '    '\n",
      "Salida: {'tasks_to_do': {'weather': False, 'exchange': False, 'news': False}, 'results': {}, 'task_completed': {}, 'error': {}, 'order_task': {}, 'ready_to_aggregate': False}\n",
      "----------------------------------------\n",
      "Entrada: '¬øC√≥mo va el clima pero no s√© si te refieres a Ciudad de M√©xico?'\n",
      "Salida: {'tasks_to_do': {'weather': True, 'exchange': False, 'news': False}, 'results': {}, 'task_completed': {}, 'error': {}, 'order_task': {}, 'ready_to_aggregate': False}\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ----- Ejemplo de prueba de llamada -----\n",
    "for input_text in test_inputs:\n",
    "    state = {\n",
    "        \"messages\": [HumanMessage(content=input_text)],\n",
    "    }\n",
    "    \n",
    "    output = classify_tasks(state)\n",
    "    \n",
    "    print(f\"Entrada: '{input_text}'\")\n",
    "    # Verificamos si el mensaje final tiene un contenido v√°lido\n",
    "    print(f\"Salida: {output}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a44be84-5cc0-4150-a600-2db5dcfe4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ver salida completa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4f63316-a33b-4558-bb6b-fdd5c0a0e83a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tasks_to_do': {'weather': True, 'exchange': False, 'news': False},\n",
       " 'results': {},\n",
       " 'task_completed': {},\n",
       " 'error': {},\n",
       " 'order_task': {},\n",
       " 'ready_to_aggregate': False}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5e74a2-ec2d-4e53-b466-e3ffa4e7b7a5",
   "metadata": {},
   "source": [
    "3. **Eval√∫a resultados**\n",
    "   - ¬øEl modelo identific√≥ correctamente las tareas?\n",
    "   - ¬øLa salida sigue el formato esperado?\n",
    "   - ¬øHay alg√∫n comportamiento inesperado?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85914b3b-6678-4016-ae5c-a1aff9cebea1",
   "metadata": {},
   "source": [
    "\n",
    "### üîç An√°lisis r√°pido del caso\n",
    "\n",
    "**Entrada:** \"El d√≥lar est√° cayendo, pero no s√© sobre el clima\"\n",
    "\n",
    "**Salida:**  \n",
    "```json\n",
    "{\n",
    "  \"weather\": false,\n",
    "  \"divisas\": true,\n",
    "  \"noticias\": true\n",
    "}\n",
    "```\n",
    "\n",
    "**Problema:**  \n",
    "El modelo activ√≥ `noticias` simplemente por la menci√≥n de una *situaci√≥n econ√≥mica*, aunque no se haya pedido expl√≠citamente noticias. Esto revela que est√° interpretando m√°s all√° del *task intent*, y no tanto como una consulta.\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Estrategias de Prompting para Mejorar Clasificaci√≥n\n",
    "\n",
    "\n",
    "\n",
    "#### 1. **Prompt con desambiguaci√≥n expl√≠cita por intenci√≥n**\n",
    "En lugar de s√≥lo pedir que clasifique temas, puedes instruir al modelo a buscar **intenciones expl√≠citas de consulta**, no solo menciones.\n",
    "\n",
    "```text\n",
    "Clasifica si el usuario QUIERE informaci√≥n sobre clima, divisas o noticias. \n",
    "No respondas True si solo se menciona el tema, sino si hay una intenci√≥n clara de solicitarlo.\n",
    "```\n",
    "\n",
    "Esto ayuda a evitar que se activen `True` por frases como ‚Äúno s√© sobre el clima‚Äù, ya que no est√° pidiendo informaci√≥n, solo haciendo referencia.\n",
    "\n",
    "Va, aqu√≠ va un resumen m√°s breve y directo, con las estrategias bien concentradas:\n",
    "\n",
    "2. **Few-shot con ejemplos ambiguos**  \n",
    "   Dar ejemplos donde hay menci√≥n sin petici√≥n para que el modelo aprenda a distinguir.\n",
    "\n",
    "3. **Preprocesamiento o reformulaci√≥n por un agente previo**  \n",
    "   Un agente intermedio puede reestructurar el mensaje del usuario para alinear mejor con el objetivo de clasificaci√≥n (quitando ruido, reordenando, o aclarando ambig√ºedades).\n",
    "\n",
    "4. **Edici√≥n del prompt para filtrar partes irrelevantes**  \n",
    "   Por ejemplo, eliminar frases como ‚Äúel d√≥lar est√° cayendo‚Äù si no implican una petici√≥n directa, o convertir \"pero no s√© sobre el clima\" en \"quiero saber el clima\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cfb0ac-b95a-4bdb-8d7c-d6699d6585b0",
   "metadata": {},
   "source": [
    "\n",
    "### üîç An√°lisis r√°pido del caso\n",
    "\n",
    "**Entrada:** \"El d√≥lar est√° cayendo, pero no s√© sobre el clima\"\n",
    "\n",
    "**Salida:**  \n",
    "```json\n",
    "{\n",
    "  \"weather\": false,\n",
    "  \"divisas\": true,\n",
    "  \"noticias\": true\n",
    "}\n",
    "```\n",
    "\n",
    "**Problema:**  \n",
    "El modelo activ√≥ `noticias` simplemente por la menci√≥n de una *situaci√≥n econ√≥mica*, aunque no se haya pedido expl√≠citamente noticias. Esto revela que est√° interpretando m√°s all√° del *task intent*, y no tanto como una consulta.\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Estrategias de Prompting para Mejorar Clasificaci√≥n\n",
    "\n",
    "\n",
    "\n",
    "#### 1. **Prompt con desambiguaci√≥n expl√≠cita por intenci√≥n**\n",
    "En lugar de s√≥lo pedir que clasifique temas, puedes instruir al modelo a buscar **intenciones expl√≠citas de consulta**, no solo menciones.\n",
    "\n",
    "```text\n",
    "Clasifica si el usuario QUIERE informaci√≥n sobre clima, divisas o noticias. \n",
    "No respondas True si solo se menciona el tema, sino si hay una intenci√≥n clara de solicitarlo.\n",
    "```\n",
    "\n",
    "Esto ayuda a evitar que se activen `True` por frases como ‚Äúno s√© sobre el clima‚Äù, ya que no est√° pidiendo informaci√≥n, solo haciendo referencia.\n",
    "\n",
    "Va, aqu√≠ va un resumen m√°s breve y directo, con las estrategias bien concentradas:\n",
    "\n",
    "2. **Few-shot con ejemplos ambiguos**  \n",
    "   Dar ejemplos donde hay menci√≥n sin petici√≥n para que el modelo aprenda a distinguir.\n",
    "\n",
    "3. **Preprocesamiento o reformulaci√≥n por un agente previo**  \n",
    "   Un agente intermedio puede reestructurar el mensaje del usuario para alinear mejor con el objetivo de clasificaci√≥n (quitando ruido, reordenando, o aclarando ambig√ºedades).\n",
    "\n",
    "4. **Edici√≥n del prompt para filtrar partes irrelevantes**  \n",
    "   Por ejemplo, eliminar frases como ‚Äúel d√≥lar est√° cayendo‚Äù si no implican una petici√≥n directa, o convertir \"pero no s√© sobre el clima\" en \"quiero saber el clima\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b671054-3c0c-4c07-b1f3-a9c6874d02c5",
   "metadata": {},
   "source": [
    "\n",
    "### Prototipado del Nodo \"Clima\" y su Prompt\n",
    "\n",
    "En esta etapa se realiza el prototipado del nodo **get_weather**, que tiene como objetivo extraer el nombre de la ciudad de la entrada del usuario y consultar la API de OpenWeather para obtener el clima de dicha ciudad. Adem√°s, se prueba y valida el dise√±o del **prompt** que gu√≠a al modelo LLM para extraer correctamente el nombre de la ciudad.\n",
    "\n",
    "üéØ **Objetivo de la prueba**  \n",
    "Validar el funcionamiento del nodo **get_weather**, asegurando que:\n",
    "\n",
    "- El prompt sea efectivo para extraer el nombre correcto de la ciudad desde un texto de entrada del usuario.\n",
    "- El nodo maneje correctamente los errores como entradas mal formadas, ciudades no reconocidas o problemas con la API de OpenWeather.\n",
    "- La respuesta tenga la estructura esperada con la informaci√≥n del clima o un mensaje de error adecuado.\n",
    "\n",
    "üß† **L√≥gica del Nodo**  \n",
    "**get_weather** debe:\n",
    "\n",
    "1. Recibir una entrada de usuario con un mensaje de texto (por ejemplo, \"¬øC√≥mo est√° el clima en Nueva York?\").\n",
    "2. Usar un **prompt** para extraer el nombre de la ciudad en ingl√©s de dicho texto.\n",
    "3. Consultar la API de OpenWeather con el nombre de la ciudad extra√≠da.\n",
    "4. Devolver un diccionario con:\n",
    "   ```python\n",
    "   {\n",
    "       \"results\": [\"Descripci√≥n del clima\"],\n",
    "       \"error\": None o mensaje de error\n",
    "   }\n",
    "   ```\n",
    "   En caso de error (por ejemplo, problemas con la API o ciudad no encontrada), el campo \"error\" debe contener una descripci√≥n detallada, y \"results\" estar√° vac√≠o.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbfd8351-4736-4bca-8e12-19fc9a629a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile agents/weather_agent.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "import logging\n",
    "from typing import Optional\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from utils.logging import setup_logging\n",
    "\n",
    "# Initialize logger using the setup_logging function\n",
    "logger = setup_logging()\n",
    "\n",
    "# ----- Load environment variables -----\n",
    "load_dotenv(dotenv_path='env')\n",
    "\n",
    "# Global LLM instance\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# ----- Prompt Template -----\n",
    "city_extraction_template = \"\"\"\n",
    "Eres un asistente que extrae el nombre de la ciudad en ingl√©s americano del siguiente texto. \n",
    "Responde solo con el nombre de la ciudad, sin comillas ni s√≠mbolos extra.\n",
    "\n",
    "Ejemplo:\n",
    "Texto: \"¬øC√≥mo est√° el clima en Nueva York?\" -> New York\n",
    "Texto: \"{text}\"\n",
    "\"\"\"\n",
    "\n",
    "city_extraction_prompt = PromptTemplate(input_variables=[\"text\"], template=city_extraction_template)\n",
    "\n",
    "# ----- LLM-based city extractor -----\n",
    "def extract_city_with_llm(text: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extracts the name of the city from the given text using a language model.\n",
    "\n",
    "    Args:\n",
    "    - text (str): The input text that may contain a city name.\n",
    "\n",
    "    Returns:\n",
    "    - str or None: Returns the city name if successfully extracted, otherwise None.\n",
    "    \"\"\"\n",
    "    logger.debug(f\"Extracting city from text: '{text}'\")\n",
    "    prompt = city_extraction_prompt.format(text=text)\n",
    "\n",
    "    try:\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "        city = response.content.strip()\n",
    "        logger.info(f\"City extracted: '{city}'\")\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Error invoking the model for city extraction.\")\n",
    "        return None\n",
    "\n",
    "    if not city or len(city) < 2 or any(c in city for c in ['{', '}', '[', ']']):\n",
    "        logger.warning(f\"Invalid city detected: '{city}'\")\n",
    "        return None\n",
    "\n",
    "    return city\n",
    "\n",
    "# ----- Weather Node -----\n",
    "def get_weather(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Handles weather-related queries using an LLM to extract the city and the OpenWeatherMap API to fetch weather data.\n",
    "\n",
    "    Returns:\n",
    "    - 'results': If successful, a dictionary with the weather report.\n",
    "    - 'error': If an issue occurs, a dictionary with the error message.\n",
    "    - 'task_completed': A boolean flag indicating if the task was completed.\n",
    "    \"\"\"\n",
    "    # A√±adir trazabilidad del nodo\n",
    "    state.setdefault(\"history\", []).append(\"task_weather\")\n",
    "\n",
    "    try:\n",
    "        input_text = state[\"messages\"][-1].content\n",
    "        logger.debug(f\"Received weather message: '{input_text}'\")\n",
    "\n",
    "        city = extract_city_with_llm(input_text)\n",
    "        logger.debug(f\"Respose llm: '{city}'\")\n",
    "\n",
    "        if not city:\n",
    "            msg = \"City could not be identified in the message.\"\n",
    "            logger.warning(msg)\n",
    "            return {\n",
    "                \"error\": {\"weather\": msg},\n",
    "                \"task_completed\": {\"weather\": False}\n",
    "            }\n",
    "\n",
    "        api_key = os.getenv(\"OPENWEATHER_API_KEY\")\n",
    "        if not api_key:\n",
    "            msg = \"API Key not configured in the system.\"\n",
    "            logger.error(msg)\n",
    "            return {\n",
    "                \"error\": {\"weather\": msg},\n",
    "                \"task_completed\": {\"weather\": False}\n",
    "            }\n",
    "\n",
    "        logger.info(f\"Fetching weather for: {city}\")\n",
    "        location_url = \"https://api.openweathermap.org/data/2.5/weather\"\n",
    "        params = {\n",
    "            \"q\": city,\n",
    "            \"appid\": api_key,\n",
    "            \"units\": \"metric\"\n",
    "        }\n",
    "        location_response = requests.get(location_url, params=params)\n",
    "\n",
    "        if location_response.status_code != 200:\n",
    "            msg = f\"City '{city}' not found or not correctly written in English.\"\n",
    "            logger.warning(msg)\n",
    "            return {\n",
    "                \"error\": {\"weather\": msg},\n",
    "                \"task_completed\": {\"weather\": False}\n",
    "            }\n",
    "\n",
    "        location_data = location_response.json()\n",
    "\n",
    "        try:\n",
    "            weather_desc = location_data[\"weather\"][0][\"description\"]\n",
    "            temperature = location_data[\"main\"][\"temp\"]\n",
    "        except (KeyError, IndexError, TypeError) as e:\n",
    "            msg = \"Unexpected weather data format received from API.\"\n",
    "            logger.exception(msg)\n",
    "            return {\n",
    "                \"error\": {\"weather\": msg},\n",
    "                \"task_completed\": {\"weather\": False}\n",
    "            }\n",
    "\n",
    "        weather_report = f\"The weather in {city} is {weather_desc} with a temperature of {temperature}¬∞C.\"\n",
    "        logger.info(f\"Generated weather report: {weather_report}\")\n",
    "\n",
    "        return {\n",
    "            \"results\": {\"weather\": [weather_report]},\n",
    "            \"task_completed\": {\"weather\": True}\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        msg = f\"Error obtaining weather: {str(e)}\"\n",
    "        logger.exception(msg)\n",
    "        return {\n",
    "            \"error\": {\"weather\": msg},\n",
    "            \"task_completed\": {\"weather\": False}\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4da0fae6-5e64-4921-a019-cc54dccb9723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Ejemplo de prueba con distintos casos -----\n",
    "test_inputs = [\n",
    "    # Casos comunes\n",
    "    \"¬øC√≥mo est√° el clima en Nueva York?\",  # Ciudad conocida\n",
    "    \"¬øCu√°l es el clima en Los √Ångeles?\",  # Ciudad con nombre compuesto\n",
    "    \"Me gustar√≠a saber c√≥mo est√° el clima en Londres.\",  # Ciudad con nombre en ingl√©s\n",
    "    \"¬øQu√© tal el clima en Ciudad de M√©xico?\",  # Ciudad con nombre compuesto en espa√±ol\n",
    "\n",
    "    # Casos especiales\n",
    "    \"¬øQu√© tal est√° el clima?\",  # Sin menci√≥n de ciudad\n",
    "    \"El clima en 1234 podr√≠a cambiar.\",  # Texto con n√∫meros, no ciudad\n",
    "    \"¬øC√≥mo est√° el clima en Paris#@\",  # Ciudad con caracteres especiales\n",
    "    \"Hace calor en Barcelon\",  # Ciudad con nombre parcialmente correcto\n",
    "    \"¬øC√≥mo est√° el clima en Shanghai?\",  # Ciudad conocida en otro idioma\n",
    "    \"¬øEn qu√© estado est√° el clima de Berl√≠n?\",  # Pregunta con \"estado\", no ciudad\n",
    "    \"¬øCu√°nto falta para llegar a Tokyo? Me olvid√© de la hora.\",  # Ciudad mencionada con informaci√≥n extra\n",
    "    \"¬øC√≥mo est√° el clima en Saint Petersburg?\",  # Ciudad compuesta con un 'Saint'\n",
    "    \"¬øC√≥mo est√° el clima en Amsterdam?\",  # Ciudad en ingl√©s sin acento\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9c65ff1-91bd-4c12-9989-e022636edd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada: '¬øC√≥mo est√° el clima en Nueva York?'\n",
      "Salida: {'results': {'weather': ['The weather in New York is clear sky with a temperature of 22.1¬∞C.']}, 'task_completed': {'weather': True}}\n",
      "Entrada: '¬øCu√°l es el clima en Los √Ångeles?'\n",
      "Salida: {'results': {'weather': ['The weather in Los Angeles is overcast clouds with a temperature of 15.93¬∞C.']}, 'task_completed': {'weather': True}}\n",
      "Entrada: 'Me gustar√≠a saber c√≥mo est√° el clima en Londres.'\n",
      "Salida: {'results': {'weather': ['The weather in Londres is clear sky with a temperature of 9.96¬∞C.']}, 'task_completed': {'weather': True}}\n",
      "Entrada: '¬øQu√© tal el clima en Ciudad de M√©xico?'\n",
      "Salida: {'results': {'weather': ['The weather in Mexico City is broken clouds with a temperature of 26.64¬∞C.']}, 'task_completed': {'weather': True}}\n",
      "Entrada: '¬øQu√© tal est√° el clima?'\n",
      "Salida: {'error': {'weather': \"City 'No city mentioned' not found or not correctly written in English.\"}, 'task_completed': {'weather': False}}\n",
      "Entrada: 'El clima en 1234 podr√≠a cambiar.'\n",
      "Salida: {'results': {'weather': ['The weather in 1234 is scattered clouds with a temperature of 17.47¬∞C.']}, 'task_completed': {'weather': True}}\n",
      "Entrada: '¬øC√≥mo est√° el clima en Paris#@'\n",
      "Salida: {'results': {'weather': ['The weather in Paris is broken clouds with a temperature of 12.27¬∞C.']}, 'task_completed': {'weather': True}}\n",
      "Entrada: 'Hace calor en Barcelon'\n",
      "Salida: {'results': {'weather': ['The weather in Barcelona is clear sky with a temperature of 16.7¬∞C.']}, 'task_completed': {'weather': True}}\n",
      "Entrada: '¬øC√≥mo est√° el clima en Shanghai?'\n",
      "Salida: {'results': {'weather': ['The weather in Shanghai is few clouds with a temperature of 16.92¬∞C.']}, 'task_completed': {'weather': True}}\n",
      "Entrada: '¬øEn qu√© estado est√° el clima de Berl√≠n?'\n",
      "Salida: {'results': {'weather': ['The weather in Berlin is clear sky with a temperature of 13.29¬∞C.']}, 'task_completed': {'weather': True}}\n",
      "Entrada: '¬øCu√°nto falta para llegar a Tokyo? Me olvid√© de la hora.'\n",
      "Salida: {'results': {'weather': ['The weather in Tokyo is mist with a temperature of 17.75¬∞C.']}, 'task_completed': {'weather': True}}\n",
      "Entrada: '¬øC√≥mo est√° el clima en Saint Petersburg?'\n",
      "Salida: {'results': {'weather': ['The weather in Saint Petersburg is overcast clouds with a temperature of 7.26¬∞C.']}, 'task_completed': {'weather': True}}\n",
      "Entrada: '¬øC√≥mo est√° el clima en Amsterdam?'\n",
      "Salida: {'results': {'weather': ['The weather in Amsterdam is few clouds with a temperature of 13.11¬∞C.']}, 'task_completed': {'weather': True}}\n"
     ]
    }
   ],
   "source": [
    "# ----- Ejemplo de prueba con el ciclo -----\n",
    "for input_text in test_inputs:\n",
    "    state = {\n",
    "                \"messages\": [HumanMessage(content=input_text)],\n",
    "                \"tasks_to_do\": {},\n",
    "                \"results\": {},\n",
    "                \"error\": {},\n",
    "                \"order_task\": None,\n",
    "                \"ready_to_aggregate\": False,\n",
    "            }\n",
    "    # Llamada al nodo de clima\n",
    "    output = get_weather(state)\n",
    "    \n",
    "    print(f\"Entrada: '{input_text}'\")\n",
    "    print(f\"Salida: {output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a59913c-9886-41d5-8588-e2359279a350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': {'weather': ['The weather in Amsterdam is few clouds with a temperature of 13.11¬∞C.']},\n",
       " 'task_completed': {'weather': True}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41276a78-020b-4d8c-9954-b5ffe679cc5c",
   "metadata": {},
   "source": [
    "\n",
    "### Consideraciones para el uso de LLMs para la extracci√≥n de ciudades\n",
    "\n",
    "Aunque los LLMs son bastante efectivos para extraer informaci√≥n de textos, hay ciertos aspectos que pueden influir en la precisi√≥n de la extracci√≥n, especialmente cuando se trata de errores tipogr√°ficos o casos especiales. A continuaci√≥n se presentan algunos puntos clave a considerar:\n",
    "\n",
    "- **Errores tipogr√°ficos menores**: Los LLMs suelen manejar errores tipogr√°ficos leves (por ejemplo, \"Nwe York\" en lugar de \"New York\"), pero pueden fallar si los errores son m√°s significativos (como \"Nuw York\" o \"New Yrok\").\n",
    "  \n",
    "- **Casos de ciudades hom√≥nimas**: En situaciones donde hay varias ciudades con el mismo nombre (como \"Paris\" en EE. UU. y \"Par√≠s\" en Francia), un LLM puede no distinguir correctamente entre ellas, especialmente si el contexto es ambiguo.\n",
    "\n",
    "- **Contexto ambiguo**: Si el contexto en el texto no es claro o hay m√∫ltiples interpretaciones posibles (por ejemplo, \"La ciudad de M√©xico\"), el LLM podr√≠a extraer una ciudad incorrecta o no estar seguro de qu√© ciudad se menciona.\n",
    "\n",
    "- **Limitaci√≥n en la precisi√≥n**: Aunque son robustos, los LLMs no siempre son infalibles para identificar la ciudad con precisi√≥n, especialmente cuando hay nombres de ciudades poco comunes o errores m√°s complejos en el texto.\n",
    "\n",
    "#### Posible mejora:\n",
    "Se podr√≠a implementar un sistema **NERD (Reconocimiento de Entidades Nombradas)** especializado en ciudades para manejar estos casos con mayor precisi√≥n, ya que est√° dise√±ado para identificar entidades espec√≠ficas de manera m√°s confiable que los modelos de lenguaje general."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d366ae95-dbe9-4eba-baca-a36e405888ed",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Prototipado del Nodo \"Divisas\" y su Prompt\n",
    "\n",
    "En esta etapa se realiza el prototipado del nodo **get_exchange_rate**, cuyo objetivo es obtener la tasa de cambio entre el d√≥lar estadounidense (USD) y el euro (EUR) usando la API de ExchangeRate. Adem√°s, se valida el dise√±o del **prompt** que guiar√° al modelo para realizar esta consulta correctamente.\n",
    "\n",
    "üéØ **Objetivo de la prueba**  \n",
    "Validar el funcionamiento del nodo **get_exchange_rate**, asegurando que:\n",
    "\n",
    "- El nodo realice la consulta correctamente a la API de ExchangeRate.\n",
    "- El manejo de errores sea adecuado en caso de problemas con la API o de respuesta inesperada.\n",
    "- El resultado tenga la estructura esperada, mostrando la tasa de cambio de USD a EUR.\n",
    "\n",
    "üß† **L√≥gica del Nodo**  \n",
    "**get_exchange_rate** debe:\n",
    "\n",
    "1. Consultar la API de ExchangeRate con la clave de API configurada.\n",
    "2. Obtener la tasa de cambio entre USD y EUR.\n",
    "3. Devolver un diccionario con:\n",
    "   ```python\n",
    "   {\n",
    "       \"results\": [\"1 USD = X EUR\"],\n",
    "       \"error\": None o mensaje de error\n",
    "   }\n",
    "   ```\n",
    "   En caso de error (como problemas con la API), el campo \"error\" debe contener la descripci√≥n detallada, y \"results\" estar√° vac√≠o.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac3008ba-a086-4d07-9c58-d67e0314d947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile agents/currency_agent.py\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import requests\n",
    "from typing import Optional\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ----- Configure logging -----\n",
    "from utils.logging import setup_logging\n",
    "\n",
    "# Initialize logger using the setup_logging function\n",
    "logger = setup_logging()\n",
    "\n",
    "# ----- Load environment variables -----\n",
    "# This loads environment variables from a .env file, typically containing sensitive information like API keys.\n",
    "load_dotenv(dotenv_path='env')\n",
    "\n",
    "# ----- Global LLM instance -----\n",
    "# Initializes the language model for currency extraction. \n",
    "# The model used here is GPT-3.5-turbo, which is ideal for text processing and extraction tasks.\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# ----- Prompt Template for currency extraction -----\n",
    "# This is the template used to instruct the language model to extract currency codes (ISO 4217 format) from the given text.\n",
    "# The prompt is written in Spanish but will be used to parse any input text in the same format.\n",
    "currency_extraction_template = \"\"\"\n",
    "Eres un asistente que extrae dos c√≥digos de divisas (ISO 4217) desde el texto dado. \n",
    "Responde √∫nicamente con los c√≥digos separados por coma. No uses s√≠mbolos ni explicaciones.\n",
    "\n",
    "Ejemplo:\n",
    "Texto: \"¬øCu√°nto vale un d√≥lar en pesos mexicanos?\" -> USD, MXN\n",
    "Texto: \"{text}\"\n",
    "\"\"\"\n",
    "\n",
    "currency_extraction_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],  # The input variable that will be passed to the template is 'text'.\n",
    "    template=currency_extraction_template  # The prompt template for extraction.\n",
    ")\n",
    "\n",
    "# ----- Function to extract currencies using the language model (LLM) -----\n",
    "def extract_currencies_with_llm(text: str) -> Optional[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Extracts two currency codes from a given text using the pre-defined language model prompt.\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): The input text containing the currencies to be extracted.\n",
    "    \n",
    "    Returns:\n",
    "    Optional[tuple[str, str]]: A tuple containing two ISO 4217 currency codes (or None if extraction fails).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Format the prompt with the provided text\n",
    "        prompt = currency_extraction_prompt.format(text=text)\n",
    "        logger.debug(f\"Prompt sent to LLM: {prompt}\")\n",
    "        \n",
    "        # Get the response from the LLM\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "        result = response.content.strip()\n",
    "        logger.debug(f\"LLM response: {result}\")\n",
    "\n",
    "        # Split the result into two parts (currency codes)\n",
    "        parts = [p.strip().upper() for p in result.split(\",\")]\n",
    "\n",
    "        # Check if the result contains exactly two 3-letter currency codes\n",
    "        if len(parts) == 2 and all(len(code) == 3 for code in parts):\n",
    "            logger.info(f\"Successfully extracted currency codes: {parts}\")\n",
    "            return parts[0], parts[1]\n",
    "        else:\n",
    "            logger.warning(f\"Unexpected format in the response: {result}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log the error if extraction fails\n",
    "        logger.exception(\"Error during currency extraction\")\n",
    "\n",
    "    return None\n",
    "\n",
    "# ----- Function to get exchange rate -----\n",
    "def get_exchange_rate(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Processes the user's message to detect currencies and fetches the exchange rate between them.\n",
    "    \n",
    "    Parameters:\n",
    "    state (dict): The current state of the agent, which contains the user's message and other context.\n",
    "    \n",
    "    Returns:\n",
    "    dict: The updated state dictionary containing either the results or error messages.\n",
    "    \"\"\"\n",
    "    # A√±adir trazabilidad del nodo\n",
    "    state.setdefault(\"history\", []).append(\"task_exchange\")\n",
    "    try:\n",
    "        # Get the input text from the state (the user's message)\n",
    "        input_text = state[\"messages\"][-1].content\n",
    "        logger.info(f\"Processing user message: {input_text}\")\n",
    "\n",
    "        # Extract the currency codes from the user's input\n",
    "        currencies = extract_currencies_with_llm(input_text)\n",
    "        \n",
    "        # If no currencies are detected, return an error\n",
    "        if not currencies:\n",
    "            msg = \"No currencies detected in the message.\"\n",
    "            logger.warning(msg)\n",
    "            return {\n",
    "                \"error\": {\"exchange\": msg},\n",
    "                \"task_completed\":{\"exchange\": True} \n",
    "            }\n",
    "\n",
    "        # Extract base and target currencies\n",
    "        base_currency, target_currency = currencies\n",
    "        logger.info(f\"Detected currencies: {base_currency} -> {target_currency}\")\n",
    "\n",
    "        # Retrieve the API key for the exchange rate service from environment variables\n",
    "        api_key = os.getenv(\"EXCHANGE_API_KEY\")\n",
    "        if not api_key:\n",
    "            logger.error(\"API Key not set in the environment.\")\n",
    "            return {\n",
    "                \"error\": {\"exchange\": \"API Key not configured in the system.\"},\n",
    "                \"task_completed\":{\"exchange\": True} \n",
    "            }\n",
    "\n",
    "        # Construct the API URL to get the exchange rates\n",
    "        url = f\"https://v6.exchangerate-api.com/v6/{api_key}/latest/{base_currency}\"\n",
    "        logger.debug(f\"Querying external API: {url}\")\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the response from the API is successful\n",
    "        if response.status_code != 200:\n",
    "            logger.error(f\"Error in API response: {response.status_code}\")\n",
    "            return {\n",
    "                \"error\": {\"exchange\": f\"API error: {response.status_code}\"},\n",
    "                \"task_completed\":{\"exchange\": False} \n",
    "            }\n",
    "\n",
    "        # Parse the JSON data from the API response\n",
    "        data = response.json()\n",
    "\n",
    "        # Check if the target currency's exchange rate is available\n",
    "        if target_currency not in data.get('conversion_rates', {}):\n",
    "            logger.warning(f\"Exchange rate for {target_currency} not available in the response.\")\n",
    "            return {\n",
    "                \"error\": {\"exchange\": f\"Exchange rate for {target_currency} not found.\"},\n",
    "                \"task_completed\":{\"exchange\": True} \n",
    "            }\n",
    "\n",
    "        # Get the exchange rate for the target currency and format the response message\n",
    "        rate = data['conversion_rates'][target_currency]\n",
    "        message = f\"1 {base_currency} = {rate} {target_currency}\"\n",
    "        logger.info(f\"Exchange rate obtained: {message}\")\n",
    "\n",
    "        # Return the exchange rate result in the updated state\n",
    "        return {\n",
    "            \"results\": {\"exchange\": [message]},\n",
    "            \"task_completed\":{\"exchange\": True} \n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any unexpected errors and return them in the state\n",
    "        logger.exception(\"Unexpected error while getting exchange rate\")\n",
    "        return {\n",
    "            \"error\": {\"exchange\": f\"Error obtaining exchange rate: {str(e)}\"},\n",
    "            \"task_completed\":{\"exchange\": False} \n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "163c7629-14b1-4246-a894-82232c67f4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Entrada: 'Quiero saber la tasa de cambio de Estados Unidos y tambi√©n el clima de Nueva York.'\n",
      "üß™ output: '{'results': {'exchange': ['1 USD = 19.6192 MXN']}, 'task_completed': {'exchange': True}}'\n",
      "üß™ Entrada: '¬øCu√°l es la tasa de cambio de euros y c√≥mo est√° el clima en Londres?'\n",
      "üß™ output: '{'results': {'exchange': ['1 EUR = 0.8567 GBP']}, 'task_completed': {'exchange': True}}'\n",
      "üß™ Entrada: 'Me interesa saber cu√°nto est√° el d√≥lar en euros y qu√© tiempo hace en Par√≠s.'\n",
      "üß™ output: '{'results': {'exchange': ['1 USD = 0.8764 EUR']}, 'task_completed': {'exchange': True}}'\n",
      "üß™ Entrada: 'Quiero la tasa de cambio entre el d√≥lar y el euro, adem√°s del clima en Tokio.'\n",
      "üß™ output: '{'results': {'exchange': ['1 USD = 0.8764 EUR']}, 'task_completed': {'exchange': True}}'\n",
      "üß™ Entrada: '¬øC√≥mo est√° el clima en Madrid y qu√© tal la tasa de cambio del d√≥lar?'\n",
      "üß™ output: '{'results': {'exchange': ['1 USD = 0.8764 EUR']}, 'task_completed': {'exchange': True}}'\n",
      "üß™ Entrada: 'Dime la tasa de cambio de Europa y el clima de Los √Ångeles.'\n",
      "üß™ output: '{'results': {'exchange': ['1 EUR = 1.1411 USD']}, 'task_completed': {'exchange': True}}'\n",
      "üß™ Entrada: 'Quiero saber la tasa de cambio de Europa y c√≥mo est√° el clima en Barcelona.'\n",
      "üß™ output: '{'results': {'exchange': ['1 EUR = 1.1411 USD']}, 'task_completed': {'exchange': True}}'\n",
      "üß™ Entrada: '¬øMe dices la tasa de cambio del d√≥lar a euro y tambi√©n qu√© clima hace en Buenos Aires?'\n",
      "üß™ output: '{'results': {'exchange': ['1 USD = 0.8764 EUR']}, 'task_completed': {'exchange': True}}'\n",
      "üß™ Entrada: '¬øCu√°nto est√° el euro en d√≥lares y c√≥mo est√° el clima en Ciudad de M√©xico?'\n",
      "üß™ output: '{'results': {'exchange': ['1 EUR = 1.1411 USD']}, 'task_completed': {'exchange': True}}'\n",
      "üß™ Entrada: 'Estoy buscando la tasa de cambio entre el d√≥lar y el euro, adem√°s del clima de Berl√≠n.'\n",
      "üß™ output: '{'results': {'exchange': ['1 USD = 0.8764 EUR']}, 'task_completed': {'exchange': True}}'\n"
     ]
    }
   ],
   "source": [
    "# ----- Ejemplo de prueba con el ciclo -----\n",
    "test_inputs = [\n",
    "    \"Quiero saber la tasa de cambio de Estados Unidos y tambi√©n el clima de Nueva York.\",\n",
    "    \"¬øCu√°l es la tasa de cambio de euros y c√≥mo est√° el clima en Londres?\",\n",
    "    \"Me interesa saber cu√°nto est√° el d√≥lar en euros y qu√© tiempo hace en Par√≠s.\",\n",
    "    \"Quiero la tasa de cambio entre el d√≥lar y el euro, adem√°s del clima en Tokio.\",\n",
    "    \"¬øC√≥mo est√° el clima en Madrid y qu√© tal la tasa de cambio del d√≥lar?\",\n",
    "    \"Dime la tasa de cambio de Europa y el clima de Los √Ångeles.\",\n",
    "    \"Quiero saber la tasa de cambio de Europa y c√≥mo est√° el clima en Barcelona.\",\n",
    "    \"¬øMe dices la tasa de cambio del d√≥lar a euro y tambi√©n qu√© clima hace en Buenos Aires?\",\n",
    "    \"¬øCu√°nto est√° el euro en d√≥lares y c√≥mo est√° el clima en Ciudad de M√©xico?\",\n",
    "    \"Estoy buscando la tasa de cambio entre el d√≥lar y el euro, adem√°s del clima de Berl√≠n.\"\n",
    "]\n",
    "\n",
    "for input_text in test_inputs:\n",
    "    state = {\n",
    "                \"messages\": [HumanMessage(content=input_text)],\n",
    "                \"tasks_to_do\": {},\n",
    "                \"results\": {},\n",
    "                \"error\": {},\n",
    "                \"order_task\": None,\n",
    "                \"ready_to_aggregate\": False,\n",
    "            }\n",
    "\n",
    "    output = get_exchange_rate(state)\n",
    "\n",
    "    print(f\"üß™ Entrada: '{input_text}'\")\n",
    "    print(f\"üß™ output: '{output}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95a87e9-bd11-4e56-a3e5-96edcfcf5edf",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### Prototipado del Nodo \"Noticias\" y su Prompt\n",
    "\n",
    "En esta etapa se realiza el prototipado del nodo **get_news**, cuyo objetivo es obtener los titulares m√°s importantes actuales de un pa√≠s usando la API de NewsAPI. Se valida el dise√±o del **prompt** que guiar√° al modelo para realizar la consulta y mostrar los titulares.\n",
    "\n",
    "üéØ **Objetivo de la prueba**  \n",
    "Validar el funcionamiento del nodo **get_news**, asegurando que:\n",
    "\n",
    "- El nodo realice correctamente la consulta a la API de NewsAPI.\n",
    "- El manejo de errores sea adecuado en caso de problemas con la API o respuesta inesperada.\n",
    "- El resultado tenga la estructura esperada, mostrando los titulares de las noticias m√°s recientes.\n",
    "\n",
    "üß† **L√≥gica del Nodo**  \n",
    "**get_news** debe:\n",
    "\n",
    "1. Consultar la API de NewsAPI con la clave de API configurada.\n",
    "2. Obtener los titulares de noticias en alg√∫n pais.\n",
    "3. Devolver un diccionario con:\n",
    "   ```python\n",
    "   {\n",
    "       \"results\": [\"Titular 1, Titular 2, Titular 3\"],\n",
    "       \"error\": None o mensaje de error\n",
    "   }\n",
    "   ```\n",
    "   En caso de error (como problemas con la API), el campo \"error\" debe contener la descripci√≥n detallada, y \"results\" estar√° vac√≠o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2029154-8b40-47fc-b68f-d7b41a5d2d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile agents/news_agent.py\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ----- Configure logging -----\n",
    "from utils.logging import setup_logging\n",
    "\n",
    "# Initialize logger using the setup_logging function\n",
    "logger = setup_logging()\n",
    "\n",
    "# ----- Load environment variables -----\n",
    "# Loads environment variables from the .env file, typically containing API keys like the News API key.\n",
    "load_dotenv(dotenv_path='env')\n",
    "\n",
    "# ----- Global LLM instance -----\n",
    "# This initializes the LLM (language model) from OpenAI with the \"gpt-3.5-turbo\" model.\n",
    "# It's used for natural language understanding and extracting country codes from text.\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# ----- Prompt Template for country extraction -----\n",
    "# This is the prompt used by the LLM to extract the country code (ISO 3166-1 alpha-2) from the provided text.\n",
    "# The prompt asks for a 2-letter country code and returns only that code.\n",
    "country_extraction_template = \"\"\"\n",
    "You are an assistant that extracts the country (in ISO 3166-1 alpha-2 code, like 'MX', 'US', 'FR') from the following text.\n",
    "Respond only with the country code. If no country is mentioned, respond with ' '.\n",
    "\n",
    "Text: \"{text}\"\n",
    "\"\"\"\n",
    "\n",
    "# Create a prompt template that the LLM will use.\n",
    "country_extraction_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],  # The input variable for the template is 'text'.\n",
    "    template=country_extraction_template  # The actual template for extraction.\n",
    ")\n",
    "\n",
    "# ----- Function to extract country from the text using the LLM -----\n",
    "def extract_country_with_llm(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the country code from the provided text using the LLM.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text containing a country mention.\n",
    "\n",
    "    Returns:\n",
    "    str: The ISO 3166-1 alpha-2 country code extracted from the text (or ' ' if no country is mentioned).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Format the prompt with the provided text\n",
    "        prompt = country_extraction_prompt.format(text=text)\n",
    "        logger.debug(f\"Prompt sent to LLM: {prompt}\")\n",
    "        \n",
    "        # Get the response from the LLM\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "        country = response.content.strip().lower()  # Normalize the country code (convert to lowercase)\n",
    "        logger.debug(f\"LLM response: {country}\")\n",
    "\n",
    "        # Validate that the response is a valid 2-letter country code\n",
    "        if len(country) != 2 or not country.isalpha():\n",
    "            logger.warning(\"Invalid response from LLM\")\n",
    "            return None  # Return a blank space if the response is invalid\n",
    "        return country\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log any exception that occurs during country extraction\n",
    "        logger.exception(\"Error during country extraction\")\n",
    "        return None  # Return 'None' as a default fallback\n",
    "\n",
    "# ----- News fetching function -----\n",
    "def get_news(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Processes the input text to detect the country and fetches news headlines for that country.\n",
    "\n",
    "    Parameters:\n",
    "    input_text (str): The input text containing user query or message.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the news headlines or error information.\n",
    "    \"\"\"\n",
    "    # A√±adir trazabilidad del nodo\n",
    "    state.setdefault(\"history\", []).append(\"task_news\")\n",
    "    \n",
    "    # Get the input text from the state (the user's message)\n",
    "    input_text = state[\"messages\"][-1].content\n",
    "    try:\n",
    "        logger.info(f\"Processing user message: {input_text}\")\n",
    "\n",
    "        # Extract the country code from the user's message\n",
    "        country_code = extract_country_with_llm(input_text)\n",
    "        logger.info(f\"Detected country code: {country_code}\")\n",
    "\n",
    "        # Retrieve the News API key from the environment variables\n",
    "        api_key = os.getenv(\"NEWS_API_KEY\")\n",
    "        if not api_key:\n",
    "            msg = \"News API Key is not configured.\"\n",
    "            logger.error(msg)\n",
    "            return {\n",
    "                    \"error\": {\"news\": msg},\n",
    "                   \"task_completed\": {\"news\": False}\n",
    "            }\n",
    "\n",
    "        # Construct the API URL to get news headlines for the detected country\n",
    "        url = f\"https://newsapi.org/v2/top-headlines?country={country_code}&apiKey={api_key}\"\n",
    "        logger.debug(f\"Querying News API: {url}\")\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Handle non-200 HTTP responses from the News API\n",
    "        if response.status_code != 200:\n",
    "            msg = f\"Error in News API: {response.status_code}\"\n",
    "            logger.error(msg)\n",
    "            return {\n",
    "                \"error\": {\"news\": msg},\n",
    "                \"task_completed\": {\"news\": False}\n",
    "            }\n",
    "\n",
    "        # Parse the JSON response from the News API\n",
    "        data = response.json()\n",
    "\n",
    "        # Check if the 'articles' key exists in the response and has data\n",
    "        if \"articles\" not in data or not data[\"articles\"]:\n",
    "            msg = f\"No news found for {country_code}.\"\n",
    "            logger.warning(msg)\n",
    "            return {\n",
    "                \"error\": {\"news\": msg},\n",
    "                \"task_completed\": {\"news\": False}\n",
    "                   }\n",
    "\n",
    "        # Extract the titles of the top 3 articles from the response\n",
    "        titles = \", \".join([article[\"title\"] for article in data[\"articles\"][:3]])\n",
    "        headlines = f\"Headlines in {country_code.upper()}: {titles}\"\n",
    "\n",
    "        logger.info(f\"Found headlines: {headlines}\")\n",
    "\n",
    "        # Return the news headlines as a dictionary\n",
    "        return {\"results\": {\"news\": [headlines]},\n",
    "                \"task_completed\": {\"news\": True}\n",
    "}\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any unexpected errors during the news retrieval process\n",
    "        logger.exception(\"Unexpected error while fetching news\")\n",
    "        return {\n",
    "            \"error\": {\"news\": f\"Error fetching news: {str(e)}\"},\n",
    "            \"task_completed\": {\"news\" : False}\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cb75e6-5d44-4337-972f-e852a44b2f7a",
   "metadata": {},
   "source": [
    "### Prototipado del Nodo  de gestion de errores y su Prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9875d113-76a7-412a-8dce-f16582cf4d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada: '¬øCu√°les son las noticias hoy en UK?'\n",
      "Salida: '{'error': {'news': 'No news found for uk.'}, 'task_completed': {'news': False}}'\n",
      "Entrada: 'Dame los titulares m√°s recientes de Estados Unidos.'\n",
      "Salida: '{'results': {'news': ['Headlines in US: Stock Market Today: Dow Jumps 800 Points; Gold Hits $3,500 on Tariff, Fed Worries ‚Äî Live Updates - WSJ, Harvard sues the Trump administration in escalating confrontation - The Washington Post, Google Fi is launching a $35 / month unlimited plan - The Verge']}, 'task_completed': {'news': True}}'\n",
      "Entrada: 'Noticias de Irak por favor'\n",
      "Salida: '{'error': {'news': 'No news found for iq.'}, 'task_completed': {'news': False}}'\n",
      "Entrada: '¬øQu√© ha pasado √∫ltimamente en Argentina?'\n",
      "Salida: '{'error': {'news': 'No news found for ar.'}, 'task_completed': {'news': False}}'\n",
      "Entrada: '¬øQu√© est√° sonando en las noticias?'\n",
      "Salida: '{'error': {'news': 'No news found for None.'}, 'task_completed': {'news': False}}'\n",
      "Entrada: 'Me gustar√≠a saber lo que pasa en Colombia y tambi√©n c√≥mo est√° el clima all√°.'\n",
      "Salida: '{'error': {'news': 'No news found for co.'}, 'task_completed': {'news': False}}'\n"
     ]
    }
   ],
   "source": [
    "test_inputs = [\n",
    "    \"¬øCu√°les son las noticias hoy en UK?\",\n",
    "    \"Dame los titulares m√°s recientes de Estados Unidos.\",\n",
    "    \"Noticias de Irak por favor\",\n",
    "    \"¬øQu√© ha pasado √∫ltimamente en Argentina?\",\n",
    "    \"¬øQu√© est√° sonando en las noticias?\",\n",
    "    \"Me gustar√≠a saber lo que pasa en Colombia y tambi√©n c√≥mo est√° el clima all√°.\"\n",
    "]\n",
    "\n",
    "for input_text in test_inputs:\n",
    "    state = {\n",
    "                \"messages\": [HumanMessage(content=input_text)],\n",
    "                \"tasks_to_do\": {},\n",
    "                \"results\": {},\n",
    "                \"error\": {},\n",
    "                \"order_task\": None,\n",
    "                \"ready_to_aggregate\": False,\n",
    "            }\n",
    "\n",
    "    output = get_news(state)\n",
    "\n",
    "    print(f\"Entrada: '{input_text}'\")\n",
    "    print(f\"Salida: '{output}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b684dea-01b6-436f-9fb3-9b33e4821033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile nodes/error_handler.py\n",
    "\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from core.agent_state import AgentState  # Adjust if needed\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ----- Configurar logging -----\n",
    "from utils.logging import setup_logging\n",
    "\n",
    "# Initialize logger using the setup_logging function\n",
    "logger = setup_logging()\n",
    "\n",
    "# ----- Cargar variables de entorno -----\n",
    "load_dotenv(dotenv_path='env')\n",
    "\n",
    "# Global LLM instance\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# ----- Error Interpretation Prompt -----\n",
    "error_handler_template = PromptTemplate(\n",
    "    input_variables=[\"error\", \"original_text\"],\n",
    "    template=\"\"\"\n",
    "Eres un asistente experto en interpretar errores de sistemas que consultan datos sobre clima, noticias y divisas.\n",
    "\n",
    "Mensaje original del usuario:\n",
    "\"{original_text}\"\n",
    "\n",
    "Mensaje de error del sistema:\n",
    "\"{error}\"\n",
    "\n",
    "1. Si hay nombres de ciudades, pa√≠ses o monedas abreviados (como 'UK', 'US', 'EUR'), proporci√≥nalos en su forma completa y clara.\n",
    "2. Genera una explicaci√≥n amigable del error para el usuario.\n",
    "3. Sugiere una alternativa. Por ejemplo, si no se puede obtener el clima, sugiere obtener noticias o divisas, y viceversa.\n",
    "reiterando que vuleva a hacer la peticion con la recomenacion asociada\n",
    "Devuelve solo el texto final para el usuario, no incluyas explicaciones adicionales ni estructuras.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# ----- Error Handler Node -----\n",
    "def error_handler(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Uses LLM to transform technical error messages into user-friendly suggestions.\n",
    "\n",
    "    Parameters:\n",
    "        state (AgentState): The shared graph state including error and last message.\n",
    "\n",
    "    Returns:\n",
    "        dict: {\"results\": {...}, \"task_completed\": {...}, \"error\": {...}}\n",
    "    \"\"\"\n",
    "\n",
    "    # A√±adir trazabilidad del nodo\n",
    "    state.setdefault(\"history\", []).append(\"task_error\")\n",
    "    try:\n",
    "        user_input = state[\"messages\"][-1].content if state.get(\"messages\") else \"\"\n",
    "\n",
    "        errores = state.get(\"error\", {})\n",
    "        nodo_source = next(iter(errores), \"desconocido\")  # Tomamos la primera clave\n",
    "        raw_error = errores.get(nodo_source, \"Error no especificado\")\n",
    "\n",
    "        logger.info(f\"Procesando error desde el nodo '{nodo_source}': {raw_error}\")\n",
    "\n",
    "        prompt = error_handler_template.format(\n",
    "            error=raw_error,\n",
    "            original_text=user_input\n",
    "        )\n",
    "        logger.debug(f\"Prompt generado para el LLM:\\n{prompt}\")\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "        friendly_message = response.content.strip()\n",
    "\n",
    "        # Remover la clave procesada\n",
    "        errores.pop(nodo_source, None)\n",
    "\n",
    "        logger.info(f\"Mensaje amigable generado: {friendly_message}\")\n",
    "        return {\n",
    "            \"results\": {nodo_source: [friendly_message]},\n",
    "            \"task_completed\": {nodo_source: True},\n",
    "            \"error\": { \"error\": errores } # Retornamos el dict sin la clave ya procesada\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Error en el manejador de errores\")\n",
    "        fallback_message = f\"No se pudo procesar el error autom√°ticamente. Detalles: {str(e)}\"\n",
    "        return {\n",
    "            \"results\": {\"error\": [fallback_message]},\n",
    "            \"task_completed\": {'error': True}\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0d8d4e1-4bbb-43a9-8be1-542b5d12d238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weather': ['Lo siento, hubo un error al obtener el estado del clima en el Reino Unido. La API est√° fuera de servicio por el momento. Te recomendar√≠a consultar las noticias o divisas en su lugar. Por favor, vuelve a hacer la petici√≥n con la recomendaci√≥n asociada.']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "state = {\n",
    "            \"messages\": [HumanMessage(content=\"¬øQu√© clima hay en UK?\")],\n",
    "            \"tasks_to_do\": {},\n",
    "            \"results\": {},\n",
    "            \"error\": {\n",
    "            \"weather\" : \"Error al obtener el estado del clima:  La API esta fuera de servicio por el momento}\"\n",
    "            },\n",
    "            \"order_task\": None,\n",
    "            \"ready_to_aggregate\": False,\n",
    "        }\n",
    "\n",
    "print(error_handler(state)['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54f96d04-f6a8-4f39-ad0c-7d63be281269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='¬øQu√© clima hay en UK?', additional_kwargs={}, response_metadata={})],\n",
       " 'tasks_to_do': {},\n",
       " 'results': {},\n",
       " 'error': {},\n",
       " 'order_task': None,\n",
       " 'ready_to_aggregate': False,\n",
       " 'history': ['task_error']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b86662b-59a4-41a3-8e38-32e523bdd9d3",
   "metadata": {},
   "source": [
    "### Prototipado del Nodo order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18877895-6469-44d5-ad98-645c9ef93c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile nodes/order_tasks.py\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from core.agent_state import AgentState\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ----- Configurar logging -----\n",
    "from utils.logging import setup_logging\n",
    "\n",
    "# Initialize logger using the setup_logging function\n",
    "logger = setup_logging()\n",
    "\n",
    "# ----- Cargar variables de entorno -----\n",
    "load_dotenv(dotenv_path='env')\n",
    "\n",
    "# Global LLM instance\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# ----- Task Ordering Prompt -----\n",
    "order_tasks_template = PromptTemplate(\n",
    "    input_variables=[\"tasks\", \"user_input\"],\n",
    "    template=\"\"\"\n",
    "Eres un asistente experto en coordinar tareas de un sistema que puede obtener informaci√≥n sobre clima, noticias y divisas.\n",
    "\n",
    "El usuario ha solicitado informaci√≥n sobre las siguientes tareas: {tasks}.\n",
    "\n",
    "Consulta original del usuario:\n",
    "\"{user_input}\"\n",
    "\n",
    "Analiza la intenci√≥n del usuario y determina el orden m√°s l√≥gico y √∫til en el que se deben ejecutar estas tareas. \n",
    "Devuelve √∫nicamente un objeto JSON donde cada clave sea el nombre de la tarea y su valor sea su posici√≥n en el orden en el que aparece en el texto ,\n",
    "si no aparece alguna tarea omitela de la respuesta\n",
    "por ejemplo: {{\"weather\": 1, \"exchange\": 2, \"noticias\": 3}}\n",
    "\n",
    "No incluyas ning√∫n otro texto ni explicaciones.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# ----- Task Ordering Node -----\n",
    "def order_tasks(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Orders tasks based on the user's input and their intent, helping to prioritize actions.\n",
    "\n",
    "    Parameters:\n",
    "        state (AgentState): The shared state that includes tasks and the user's query.\n",
    "\n",
    "    Returns:\n",
    "        dict: {\"results\": {\"order\": ...}, \"task_completed\": {\"order\": True}}\n",
    "    \"\"\"\n",
    "    # A√±adir trazabilidad del nodo\n",
    "    state.setdefault(\"history\", []).append(\"task_order\")\n",
    "    tasks = {k: v for k, v in state.get(\"tasks\", {}).items() if v}\n",
    "    user_input = state[\"messages\"][-1].content if state.get(\"messages\") else \"\"\n",
    "\n",
    "    logger.info(f\"Tareas detectadas: {list(tasks.keys())}\")\n",
    "    logger.debug(f\"Consulta del usuario: {user_input}\")\n",
    "\n",
    "    try:\n",
    "        prompt = order_tasks_template.format(\n",
    "            tasks=\", \".join(tasks.keys()),\n",
    "            user_input=user_input\n",
    "        )\n",
    "        logger.debug(f\"Prompt generado para el LLM:\\n{prompt}\")\n",
    "\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "        ordered_dict = json.loads(response.content.strip())\n",
    "\n",
    "        logger.info(f\"Orden propuesto por LLM: {ordered_dict}\")\n",
    "\n",
    "        return {\n",
    "            \"order_task\":  ordered_dict,\n",
    "            \"task_completed\": {'order':True}\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error al ordenar tareas: {str(e)}\"\n",
    "        logger.exception(error_msg)\n",
    "        return {\n",
    "            \"task_completed\": {'order':False},\n",
    "            \"error\": {\"order\": error_msg}\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "466503f3-63cc-440e-ae4a-c1f0573f2035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.agent_state import AgentState  # Ajusta la importaci√≥n seg√∫n tu estructura\n",
    "\n",
    "# Estado de ejemplo\n",
    "state = {\n",
    "    \n",
    "    \"messages\": [HumanMessage(content=\"Quiero saber el clima y las noticias, pero en orden de importancia\")],\n",
    "    \"tasks\": {\n",
    "        \"weather\": True,  # Tarea activa\n",
    "        \"exchange\": True,  # Tarea activa\n",
    "        \"news\": True  # Tarea activa\n",
    "    },\n",
    "    \"error\": {},\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ede11519-f5b2-493f-845f-f9b005b049ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'order_task': {'weather': 1, 'news': 2}, 'task_completed': {'order': True}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Llama la funci√≥n order_tasks\n",
    "new_state = order_tasks(state)\n",
    "\n",
    "# Imprime el nuevo estado para verificar la respuesta\n",
    "print(new_state)  # Aqu√≠ deber√≠as ver el diccionario con el orden propuesto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8460b9-f09c-4a38-87d7-25b4a427432b",
   "metadata": {},
   "source": [
    "### Prototipado del Nodo integrador\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df3d6cdb-40ec-44d5-bfc6-1a9cf9d4f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile nodes/aggregator_tasks.py\n",
    "\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from core.agent_state import AgentState  # Ajustar seg√∫n sea necesario\n",
    "\n",
    "# ----- Configurar logging -----\n",
    "from utils.logging import setup_logging\n",
    "\n",
    "# Initialize logger using the setup_logging function\n",
    "logger = setup_logging()\n",
    "\n",
    "# ----- Cargar variables de entorno -----\n",
    "load_dotenv(dotenv_path='env')\n",
    "\n",
    "# Instancia global de LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Plantilla para \"enchulamiento\"\n",
    "enchulador_template = PromptTemplate(\n",
    "    input_variables=[\"mensaje\"],\n",
    "    template=\"\"\"\n",
    "Enchula el siguiente mensaje para hacerlo amigable para el usuario:\n",
    "\n",
    "{mensaje}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "def aggregator(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Reformula resultados exitosos con el LLM y agrega errores directamente.\n",
    "    Devuelve mensajes listos para mostrar al usuario.\n",
    "\n",
    "    Args:\n",
    "        state (dict): Contiene 'order_task', 'results', 'error'.\n",
    "\n",
    "    Returns:\n",
    "        dict: {\"results\": {\"aggregator\": [messages]}, \"task_completed\":  {}}\n",
    "    \"\"\"\n",
    "    # A√±adir trazabilidad del nodo\n",
    "    state.setdefault(\"history\", []).append(\"task_aggregator\")\n",
    "    processed_messages = []\n",
    "\n",
    "    logger.info(\"Iniciando agregaci√≥n de tareas...\")\n",
    "\n",
    "    # Ordenar las tareas seg√∫n el orden dado en el diccionario order_task\n",
    "    # Ordenamos el diccionario order_task por los valores (el orden de las tareas)\n",
    "    sorted_order = sorted(state.get(\"order_task\", {}).items(), key=lambda item: item[1])\n",
    "    logger.info(\"Orden detectado  %s\", sorted_order)\n",
    "\n",
    "    # Iterar en el orden correcto\n",
    "    for task, order in sorted_order:\n",
    "        result = state.get(\"results\", {}).get(task)\n",
    "        error = state.get(\"error\", {}).get(task)\n",
    "\n",
    "        logger.debug(f\"Tarea: {task} | Orden: {order} | Resultado: {result} | Error: {error}\")\n",
    "\n",
    "\n",
    "        if result:\n",
    "            mensaje_bruto = f\"{task.capitalize()}: {result}\"\n",
    "            try:\n",
    "                prompt_text = enchulador_template.format(mensaje=mensaje_bruto)\n",
    "                prompt = HumanMessage(content=prompt_text)\n",
    "                response = llm.invoke([prompt])\n",
    "                friendly_text = response.content.strip()\n",
    "                logger.info(f\"Mensaje procesado para '{task}': {friendly_text}\")\n",
    "                processed_messages.append(friendly_text)\n",
    "            except Exception as e:\n",
    "                logger.exception(f\"No se pudo reformular el mensaje para '{task}': {str(e)}\")\n",
    "                processed_messages.append(mensaje_bruto)\n",
    "\n",
    "    # Retornar solo el formato correcto, sin modificar el state\n",
    "    return {\n",
    "        \"results\": {\"aggregator\": processed_messages},\n",
    "        \"task_completed\": {'aggregator':True}\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49e13b39-ec27-45fc-9feb-2864c29317d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulaci√≥n de estado con resultados y errores\n",
    "mock_state = {\n",
    "    \"order_task\": {  # Usamos 'order_task' con el orden de las tareas\n",
    "        \"weather\": 1,\n",
    "        \"exchange\": 2,\n",
    "        \"news\": 3\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"weather\": \"En CDMX hay 22¬∞C y cielo parcialmente nublado.\",\n",
    "        \"exchange\": \"No se pudo obtener el tipo de cambio. Verifica la moneda solicitada.\",\n",
    "        \"news\": \"Elecciones en Espa√±a dominan los titulares europeos.\"\n",
    "    },\n",
    "    \"error\": {\n",
    "\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Ejecutamos la funci√≥n\n",
    "updated_state = aggregator(mock_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0eb4df70-d501-4d3f-8097-31f55cf0bcbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': {'aggregator': ['¬°Hola! Te informo que en la Ciudad de M√©xico tenemos una temperatura de 22¬∞C y el cielo se encuentra parcialmente nublado. ¬°Que tengas un excelente d√≠a! üå§Ô∏èüå°Ô∏è',\n",
       "   '¬°Hola! Parece que no pudimos obtener el tipo de cambio para la moneda que solicitaste. Te recomendamos verificar la moneda que ingresaste. ¬°Gracias por tu comprensi√≥n!',\n",
       "   '¬°Hola! Te contamos que las elecciones en Espa√±a est√°n acaparando la atenci√≥n de los titulares europeos. ¬°No te pierdas las √∫ltimas noticias!']},\n",
       " 'task_completed': {'aggregator': True}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
