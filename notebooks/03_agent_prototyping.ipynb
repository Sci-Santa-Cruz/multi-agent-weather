{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08f76abc-3f8b-4882-bd10-5085028c352b",
   "metadata": {},
   "source": [
    "# Ingeniería de Prompts  & Prototipado del Agente\n",
    "\n",
    "#### **Objetivo**\n",
    "Desarrollar y prototipar el agente mediante el diseño de prompts efectivos y su integración en un prototipo funcional. Este notebook combina la ingeniería de prompts con el prototipado del agente, permitiendo iteraciones y pruebas rápidas.\n",
    "\n",
    "#### **Secciones**\n",
    "\n",
    "1. **Ingeniería de Prompts**\n",
    "   - Diseñar y probar diversas estructuras de prompts para asegurar que el modelo realice las tareas deseadas.\n",
    "   - Crear prompts específicos para cada responsabilidad del agente.\n",
    "   - Evaluar la efectividad de los prompts a través de pruebas iniciales.\n",
    "\n",
    "2. **Prototipado del Agente**\n",
    "   - Implementar la estructura básica del agente e integrar los prompts diseñados.\n",
    "   - Desarrollar funciones esenciales (ej., llamadas a APIs, manejo de entradas).\n",
    "   - Ejecutar pruebas iniciales y validar el rendimiento del agente.\n",
    "\n",
    "3. **Iteración y Refinamiento**\n",
    "   - Ajustar los prompts y funciones del agente según los resultados de las pruebas.\n",
    "   - Refinar el agente para mejorar su desempeño."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138948f9-a5e3-45e7-9109-be1aa5a559cc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Prototipado del Nodo `classify_tasks` y su Prompt\n",
    "\n",
    "En esta etapa se realiza el prototipado del primer componente del agente: el nodo `classify_tasks`. Este nodo es responsable de interpretar la entrada del usuario y clasificar qué tareas están presentes (clima, divisas, noticias). Además del código funcional, se prueba y valida el prompting específico que permitirá al modelo ejecutar esta clasificación de forma consistente.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🎯 Objetivo de la prueba\n",
    "\n",
    "Validar el correcto funcionamiento del nodo `classify_tasks`, incluyendo:\n",
    "\n",
    "- El diseño y comportamiento del prompt que guía al LLM para identificar tareas dentro de un `user_input`.\n",
    "- El manejo de errores en la llamada al modelo.\n",
    "- La consistencia en la estructura de la respuesta.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧠 Lógica del Nodo\n",
    "\n",
    "`classify_tasks` debe:\n",
    "\n",
    "- Recibir una entrada de usuario (`user_input`).\n",
    "- Usar un prompt diseñado para pedir al LLM que identifique si el texto menciona temas de:\n",
    "  - Clima\n",
    "  - Tipo de cambio\n",
    "  - Noticias\n",
    "- Devolver un diccionario con:\n",
    "  ```python\n",
    "  {\n",
    "      \"tasks\": {\n",
    "          \"weather\": True/False,\n",
    "          \"currency\": True/False,\n",
    "          \"news\": True/False\n",
    "      },\n",
    "      \"error\": None o mensaje de error\n",
    "  }\n",
    "  ```\n",
    "\n",
    "En caso de error (por ejemplo, problemas con el LLM), el campo `\"error\"` debe incluir una descripción del fallo, y `\"tasks\"` tendrá todos los valores en `False`.\n",
    "\n",
    "---\n",
    "\n",
    "#### Componentes de la prueba\n",
    "\n",
    "1. **Prompting**  \n",
    "   Validar que el prompt sea claro, específico y conduzca al LLM a generar una salida estructurada y predecible. Un ejemplo de prompt podría ser:\n",
    "\n",
    "   ```\n",
    "   Dada la siguiente entrada del usuario, identifica si contiene menciones sobre:\n",
    "\n",
    "   - Clima\n",
    "   - Tipo de cambio (divisas)\n",
    "   - Noticias generales\n",
    "\n",
    "   Devuelve el resultado en formato JSON como este:\n",
    "   {\n",
    "     \"tasks_to_do\": {\n",
    "       \"weather\": true/false,\n",
    "       \"currency\": true/false,\n",
    "       \"news\": true/false\n",
    "     }\n",
    "   }\n",
    "\n",
    "   Entrada: <<user_input>>\n",
    "   ```\n",
    "\n",
    "2. **Pruebas funcionales del nodo**\n",
    "   - Entradas con menciones claras de cada tipo de tarea.\n",
    "   - Entradas mixtas (clima + noticias, divisas + clima, etc.).\n",
    "   - Entradas ambiguas o vacías.\n",
    "   - Casos de error simulados (por ejemplo, lanzar una excepción desde el LLM).\n",
    "\n",
    "---\n",
    "\n",
    "#### Pasos para ejecutar la prueba\n",
    "\n",
    "1. **Configura el entorno**  \n",
    "   - Asegúrate de tener instalada tu librería LLM (por ejemplo, `openai`) y tener una clave API válida.\n",
    "   - Carga o define tu función `classify_tasks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc99bdd6-0284-4fcc-821e-90acb7f4c32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ir al directorio principal\n",
    "from os import chdir\n",
    "\n",
    "chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a782be8-1f8e-4c78-a888-e55bcaf74022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar varibles\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='env')\n",
    "\n",
    "from utils.logging import setup_logging\n",
    "\n",
    "# Initialize logger using the setup_logging function\n",
    "logger = setup_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbcfe81-1d71-4495-a5d1-1f970dfc58ec",
   "metadata": {},
   "source": [
    "### Instala las bibliotecas necesarias\n",
    "\n",
    "Ejecuta el siguiente comando en la terminal:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28b831c7-d99b-442b-8e32-6488bad16728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ----- Modelo -----\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03611ebf-6369-4a54-a176-6f92c6ebaac6",
   "metadata": {},
   "source": [
    "### Definir el `state` que compartirán los nodos y agentes del grafo\n",
    "\n",
    "Este `state` es el contenedor central de información que se irá actualizando y transmitiendo entre nodos a lo largo del flujo definido en el grafo de LangGraph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1f3b1bb-ba3c-4c5d-8145-50483a6148a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile core/agent_state.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from typing import Annotated, TypedDict, Optional, List, Dict, Any\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv(dotenv_path='env')\n",
    "\n",
    "# Función para combinar diccionarios\n",
    "def merge_dicts(dict1, dict2):\n",
    "    if not isinstance(dict1, dict) or not isinstance(dict2, dict):\n",
    "        raise TypeError(f\"Ambos argumentos deben ser diccionarios. Recibido: {type(dict1)} y {type(dict2)}\")\n",
    "    return {**dict1, **dict2}\n",
    "\n",
    "def add_history_update(history_old: List[str], history_new: List[str]) -> List[str]:\n",
    "    return history_old + history_new\n",
    "\n",
    "# Clase AgentState\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Representa el estado que fluye a través del agente LangGraph.\n",
    "\n",
    "    Atributos:\n",
    "        messages (list[BaseMessage]):\n",
    "            Lista de mensajes intercambiados entre el sistema y el usuario,\n",
    "            gestionados automáticamente con el paso de mensajes de LangGraph.\n",
    "        \n",
    "        order_task (Optional[List[str]]):\n",
    "            Lista opcional que representa el orden en el que las tareas identificadas \n",
    "            deben ser procesadas o presentadas.\n",
    "\n",
    "        results (Dict[str, Any]):\n",
    "            Diccionario que contiene los resultados de varios nodos de tareas.\n",
    "            Cada clave es el nombre de la tarea, y el valor es el resultado o error.\n",
    "\n",
    "        error (Optional[str]):\n",
    "            Mensaje de error general para representar cualquier problema en el flujo del agente.\n",
    "\n",
    "        tasks_to_do (Dict[str, bool]):\n",
    "            Diccionario que representa qué tareas han sido identificadas para ejecutar.\n",
    "            Ejemplo: {\"weather\": True, \"currencies\": True, \"news\": False}\n",
    "\n",
    "        ready_to_aggregate (bool):\n",
    "            Indica si todas las tareas esperadas están listas y el paso final de agregación puede ejecutarse.\n",
    "\n",
    "        history (List[str]):\n",
    "            Lista para realizar un seguimiento de los nombres de los nodos por los que pasa el flujo.\n",
    "    \"\"\"\n",
    "    \n",
    "    messages: Annotated[List[BaseMessage], add_messages]  # Mensajes intercambiados\n",
    "    order_task: Dict[str, Any]  # Orden de las tareas\n",
    "    error: Annotated[Dict[str, str], merge_dicts]  # Manejo de errores\n",
    "    results: Annotated[Dict[str, str], merge_dicts]  # Resultados de las tareas\n",
    "    task_completed: Annotated[Dict[str, str], merge_dicts]  # Tareas completadas\n",
    "    tasks_to_do: Dict[str, bool]  # Tareas pendientes\n",
    "    ready_to_aggregate: bool  # Indicador de si está listo para agregarse\n",
    "    history: Annotated[List[str], add_history_update]  # Historial de nodos procesados\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0786bae1-e6a9-4eac-905b-aeb366aece07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile nodes/classify_query.py\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import logging\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import cast\n",
    "from core.agent_state import AgentState\n",
    "import json\n",
    "\n",
    "from utils.logging import setup_logging\n",
    "\n",
    "# Initialize logger using the setup_logging function\n",
    "logger = setup_logging()\n",
    "\n",
    "# ----- Cargar variables de entorno -----\n",
    "load_dotenv(dotenv_path='env')\n",
    "\n",
    "# Global LLM instance\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "# ----- System message (prompt) -----\n",
    "system_prompt = SystemMessage(\n",
    "    content=\"\"\"\n",
    "Eres un asistente que clasifica tareas en tres categorías: weather, exchange y news.\n",
    "Dado un mensaje del usuario, responde con un JSON con claves: \"weather\", \"exchange\", \"news\",\n",
    "y valores booleanos indicando si la tarea está presente.\n",
    "Ejemplo de respuesta: {\"weather\": true, \"exchange\": false, \"news\": true}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# ----- Node: classify_tasks -----\n",
    "def classify_tasks(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Clasifica la intención del usuario en categorías predefinidas\n",
    "    usando un modelo de lenguaje y actualiza el estado.\n",
    "    \"\"\"\n",
    "    # Añadir trazabilidad del nodo\n",
    "    state.setdefault(\"history\", []).append(\"classify_tasks\")\n",
    "    try:\n",
    "        logger.debug(\"Buscando el último mensaje del usuario...\")\n",
    "        user_msg = [m for m in state[\"messages\"] if isinstance(m, HumanMessage)][-1]\n",
    "        logger.info(f\"Mensaje recibido: {user_msg.content}\")\n",
    "\n",
    "        full_prompt = [system_prompt, user_msg]\n",
    "\n",
    "        logger.debug(\"Enviando prompt al modelo...\")\n",
    "        response = llm.invoke(full_prompt)\n",
    "        logger.debug(f\"Respuesta del modelo: {response.content}\")\n",
    "\n",
    "        classification = json.loads(response.content)\n",
    "        classification = cast(dict, classification)\n",
    "        logger.info(f\"Tareas clasificadas: {classification}\")\n",
    "\n",
    "        new_state = {\n",
    "            \"tasks_to_do\": classification,\n",
    "            \"results\": state.get(\"results\", {}),\n",
    "            \"task_completed\": {},\n",
    "            \"error\": {},\n",
    "            \"order_task\": {},\n",
    "            \"ready_to_aggregate\": False,\n",
    "        }\n",
    "\n",
    "        logger.debug(\"Estado actualizado correctamente.\")\n",
    "        return new_state\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"classify: {str(e)}\"\n",
    "        logger.exception(\"Error al clasificar el mensaje del usuario.\")\n",
    "        return {\n",
    "            \"messages\": state[\"messages\"] + [SystemMessage(content=error_msg)],\n",
    "            \"results\": state.get(\"results\", {}),\n",
    "            \"tasks_to_do\": {},\n",
    "            \"error\": {\"classify\": error_msg},\n",
    "            \"order_task\": {},\n",
    "            \"ready_to_aggregate\": False,\n",
    "            \"task_completed\": {}\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eb68aa-d0b2-4530-9fa8-94b7b98c2853",
   "metadata": {},
   "source": [
    "2. **Ejecuta ejemplos de prueba**\n",
    "\n",
    " Define diferentes entradas de usuario para probar varios escenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c08d96e2-44ae-4acf-98e4-b49a5e9ea005",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = [\n",
    "    # Casos con claridad\n",
    "    \"¿Cómo está el clima en Ciudad de México?\",  # Clima\n",
    "    \"¿Cuál es el tipo de cambio de USD a EUR hoy?\",  # Divisas\n",
    "    \"Dame las últimas noticias de México.\",  # Noticias\n",
    "    # Casos combinados\n",
    "    \"Quiero saber el clima y las noticias.\",  # Clima, Noticias\n",
    "    \"Tipo de cambio del dólar.\",  # Divisas\n",
    "    \"Noticias del fútbol y el clima mañana.\",  # Clima, Noticias\n",
    "    # Casos ambiguos o sin relación\n",
    "    \"Algo completamente sin relación.\",  # Ninguno\n",
    "    \"No entiendo qué pasa con el clima\",  # Clima (con ambigüedad)\n",
    "    \"El dólar está cayendo, pero no sé sobre el clima\",  # Divisas\n",
    "    \"¿Hay noticias del fútbol?\",  # Noticias\n",
    "    # Casos de error\n",
    "    \"\",  # Entrada vacía\n",
    "    \"    \",  # Entrada vacía con espacios\n",
    "    \"¿Cómo va el clima pero no sé si te refieres a Ciudad de México?\"  # Clima, ambigüedad\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e0a403-b1cc-4067-a453-907b6a6b717a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8c759ca-8351-4840-af9b-f0887164d0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada: '¿Cómo está el clima en Ciudad de México?'\n",
      "Salida: {'tasks_to_do': {'weather': True, 'exchange': False, 'news': False}, 'results': {}, 'task_completed': {}, 'error': {}, 'order_task': {}, 'ready_to_aggregate': False}\n",
      "----------------------------------------\n",
      "Entrada: '¿Cuál es el tipo de cambio de USD a EUR hoy?'\n",
      "Salida: {'tasks_to_do': {'weather': False, 'exchange': True, 'news': False}, 'results': {}, 'task_completed': {}, 'error': {}, 'order_task': {}, 'ready_to_aggregate': False}\n",
      "----------------------------------------\n",
      "Entrada: 'Dame las últimas noticias de México.'\n",
      "Salida: {'tasks_to_do': {'weather': False, 'exchange': False, 'news': True}, 'results': {}, 'task_completed': {}, 'error': {}, 'order_task': {}, 'ready_to_aggregate': False}\n",
      "----------------------------------------\n",
      "Entrada: 'Quiero saber el clima y las noticias.'\n",
      "Salida: {'tasks_to_do': {'weather': True, 'exchange': False, 'news': True}, 'results': {}, 'task_completed': {}, 'error': {}, 'order_task': {}, 'ready_to_aggregate': False}\n",
      "----------------------------------------\n",
      "Entrada: 'Tipo de cambio del dólar.'\n",
      "Salida: {'tasks_to_do': {'weather': False, 'exchange': True, 'news': False}, 'results': {}, 'task_completed': {}, 'error': {}, 'order_task': {}, 'ready_to_aggregate': False}\n",
      "----------------------------------------\n",
      "Entrada: 'Noticias del fútbol y el clima mañana.'\n",
      "Salida: {'tasks_to_do': {'weather': True, 'exchange': False, 'news': True}, 'results': {}, 'task_completed': {}, 'error': {}, 'order_task': {}, 'ready_to_aggregate': False}\n",
      "----------------------------------------\n",
      "Entrada: 'Algo completamente sin relación.'\n",
      "Salida: {'tasks_to_do': {'weather': False, 'exchange': False, 'news': False}, 'results': {}, 'task_completed': {}, 'error': {}, 'order_task': {}, 'ready_to_aggregate': False}\n",
      "----------------------------------------\n",
      "Entrada: 'No entiendo qué pasa con el clima'\n",
      "Salida: {'tasks_to_do': {'weather': True, 'exchange': False, 'news': False}, 'results': {}, 'task_completed': {}, 'error': {}, 'order_task': {}, 'ready_to_aggregate': False}\n",
      "----------------------------------------\n",
      "Entrada: 'El dólar está cayendo, pero no sé sobre el clima'\n",
      "Salida: {'tasks_to_do': {'weather': False, 'exchange': True, 'news': True}, 'results': {}, 'task_completed': {}, 'error': {}, 'order_task': {}, 'ready_to_aggregate': False}\n",
      "----------------------------------------\n",
      "Entrada: '¿Hay noticias del fútbol?'\n",
      "Salida: {'tasks_to_do': {'weather': False, 'exchange': False, 'news': True}, 'results': {}, 'task_completed': {}, 'error': {}, 'order_task': {}, 'ready_to_aggregate': False}\n",
      "----------------------------------------\n",
      "Entrada: ''\n",
      "Salida: {'tasks_to_do': {'weather': False, 'exchange': False, 'news': False}, 'results': {}, 'task_completed': {}, 'error': {}, 'order_task': {}, 'ready_to_aggregate': False}\n",
      "----------------------------------------\n",
      "Entrada: '    '\n",
      "Salida: {'tasks_to_do': {'weather': False, 'exchange': False, 'news': False}, 'results': {}, 'task_completed': {}, 'error': {}, 'order_task': {}, 'ready_to_aggregate': False}\n",
      "----------------------------------------\n",
      "Entrada: '¿Cómo va el clima pero no sé si te refieres a Ciudad de México?'\n",
      "Salida: {'tasks_to_do': {'weather': True, 'exchange': False, 'news': False}, 'results': {}, 'task_completed': {}, 'error': {}, 'order_task': {}, 'ready_to_aggregate': False}\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ----- Ejemplo de prueba de llamada -----\n",
    "for input_text in test_inputs:\n",
    "    state = {\n",
    "        \"messages\": [HumanMessage(content=input_text)],\n",
    "    }\n",
    "    \n",
    "    output = classify_tasks(state)\n",
    "    \n",
    "    print(f\"Entrada: '{input_text}'\")\n",
    "    # Verificamos si el mensaje final tiene un contenido válido\n",
    "    print(f\"Salida: {output}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a44be84-5cc0-4150-a600-2db5dcfe4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ver salida completa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4f63316-a33b-4558-bb6b-fdd5c0a0e83a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tasks_to_do': {'weather': True, 'exchange': False, 'news': False},\n",
       " 'results': {},\n",
       " 'task_completed': {},\n",
       " 'error': {},\n",
       " 'order_task': {},\n",
       " 'ready_to_aggregate': False}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5e74a2-ec2d-4e53-b466-e3ffa4e7b7a5",
   "metadata": {},
   "source": [
    "3. **Evalúa resultados**\n",
    "   - ¿El modelo identificó correctamente las tareas?\n",
    "   - ¿La salida sigue el formato esperado?\n",
    "   - ¿Hay algún comportamiento inesperado?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85914b3b-6678-4016-ae5c-a1aff9cebea1",
   "metadata": {},
   "source": [
    "\n",
    "### 🔍 Análisis rápido del caso\n",
    "\n",
    "**Entrada:** \"El dólar está cayendo, pero no sé sobre el clima\"\n",
    "\n",
    "**Salida:**  \n",
    "```json\n",
    "{\n",
    "  \"weather\": false,\n",
    "  \"divisas\": true,\n",
    "  \"noticias\": true\n",
    "}\n",
    "```\n",
    "\n",
    "**Problema:**  \n",
    "El modelo activó `noticias` simplemente por la mención de una *situación económica*, aunque no se haya pedido explícitamente noticias. Esto revela que está interpretando más allá del *task intent*, y no tanto como una consulta.\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Estrategias de Prompting para Mejorar Clasificación\n",
    "\n",
    "\n",
    "\n",
    "#### 1. **Prompt con desambiguación explícita por intención**\n",
    "En lugar de sólo pedir que clasifique temas, puedes instruir al modelo a buscar **intenciones explícitas de consulta**, no solo menciones.\n",
    "\n",
    "```text\n",
    "Clasifica si el usuario QUIERE información sobre clima, divisas o noticias. \n",
    "No respondas True si solo se menciona el tema, sino si hay una intención clara de solicitarlo.\n",
    "```\n",
    "\n",
    "Esto ayuda a evitar que se activen `True` por frases como “no sé sobre el clima”, ya que no está pidiendo información, solo haciendo referencia.\n",
    "\n",
    "Va, aquí va un resumen más breve y directo, con las estrategias bien concentradas:\n",
    "\n",
    "2. **Few-shot con ejemplos ambiguos**  \n",
    "   Dar ejemplos donde hay mención sin petición para que el modelo aprenda a distinguir.\n",
    "\n",
    "3. **Preprocesamiento o reformulación por un agente previo**  \n",
    "   Un agente intermedio puede reestructurar el mensaje del usuario para alinear mejor con el objetivo de clasificación (quitando ruido, reordenando, o aclarando ambigüedades).\n",
    "\n",
    "4. **Edición del prompt para filtrar partes irrelevantes**  \n",
    "   Por ejemplo, eliminar frases como “el dólar está cayendo” si no implican una petición directa, o convertir \"pero no sé sobre el clima\" en \"quiero saber el clima\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cfb0ac-b95a-4bdb-8d7c-d6699d6585b0",
   "metadata": {},
   "source": [
    "\n",
    "### 🔍 Análisis rápido del caso\n",
    "\n",
    "**Entrada:** \"El dólar está cayendo, pero no sé sobre el clima\"\n",
    "\n",
    "**Salida:**  \n",
    "```json\n",
    "{\n",
    "  \"weather\": false,\n",
    "  \"divisas\": true,\n",
    "  \"noticias\": true\n",
    "}\n",
    "```\n",
    "\n",
    "**Problema:**  \n",
    "El modelo activó `noticias` simplemente por la mención de una *situación económica*, aunque no se haya pedido explícitamente noticias. Esto revela que está interpretando más allá del *task intent*, y no tanto como una consulta.\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Estrategias de Prompting para Mejorar Clasificación\n",
    "\n",
    "\n",
    "\n",
    "#### 1. **Prompt con desambiguación explícita por intención**\n",
    "En lugar de sólo pedir que clasifique temas, puedes instruir al modelo a buscar **intenciones explícitas de consulta**, no solo menciones.\n",
    "\n",
    "```text\n",
    "Clasifica si el usuario QUIERE información sobre clima, divisas o noticias. \n",
    "No respondas True si solo se menciona el tema, sino si hay una intención clara de solicitarlo.\n",
    "```\n",
    "\n",
    "Esto ayuda a evitar que se activen `True` por frases como “no sé sobre el clima”, ya que no está pidiendo información, solo haciendo referencia.\n",
    "\n",
    "Va, aquí va un resumen más breve y directo, con las estrategias bien concentradas:\n",
    "\n",
    "2. **Few-shot con ejemplos ambiguos**  \n",
    "   Dar ejemplos donde hay mención sin petición para que el modelo aprenda a distinguir.\n",
    "\n",
    "3. **Preprocesamiento o reformulación por un agente previo**  \n",
    "   Un agente intermedio puede reestructurar el mensaje del usuario para alinear mejor con el objetivo de clasificación (quitando ruido, reordenando, o aclarando ambigüedades).\n",
    "\n",
    "4. **Edición del prompt para filtrar partes irrelevantes**  \n",
    "   Por ejemplo, eliminar frases como “el dólar está cayendo” si no implican una petición directa, o convertir \"pero no sé sobre el clima\" en \"quiero saber el clima\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b671054-3c0c-4c07-b1f3-a9c6874d02c5",
   "metadata": {},
   "source": [
    "\n",
    "### Prototipado del Nodo \"Clima\" y su Prompt\n",
    "\n",
    "En esta etapa se realiza el prototipado del nodo **get_weather**, que tiene como objetivo extraer el nombre de la ciudad de la entrada del usuario y consultar la API de OpenWeather para obtener el clima de dicha ciudad. Además, se prueba y valida el diseño del **prompt** que guía al modelo LLM para extraer correctamente el nombre de la ciudad.\n",
    "\n",
    "🎯 **Objetivo de la prueba**  \n",
    "Validar el funcionamiento del nodo **get_weather**, asegurando que:\n",
    "\n",
    "- El prompt sea efectivo para extraer el nombre correcto de la ciudad desde un texto de entrada del usuario.\n",
    "- El nodo maneje correctamente los errores como entradas mal formadas, ciudades no reconocidas o problemas con la API de OpenWeather.\n",
    "- La respuesta tenga la estructura esperada con la información del clima o un mensaje de error adecuado.\n",
    "\n",
    "🧠 **Lógica del Nodo**  \n",
    "**get_weather** debe:\n",
    "\n",
    "1. Recibir una entrada de usuario con un mensaje de texto (por ejemplo, \"¿Cómo está el clima en Nueva York?\").\n",
    "2. Usar un **prompt** para extraer el nombre de la ciudad en inglés de dicho texto.\n",
    "3. Consultar la API de OpenWeather con el nombre de la ciudad extraída.\n",
    "4. Devolver un diccionario con:\n",
    "   ```python\n",
    "   {\n",
    "       \"results\": [\"Descripción del clima\"],\n",
    "       \"error\": None o mensaje de error\n",
    "   }\n",
    "   ```\n",
    "   En caso de error (por ejemplo, problemas con la API o ciudad no encontrada), el campo \"error\" debe contener una descripción detallada, y \"results\" estará vacío.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbfd8351-4736-4bca-8e12-19fc9a629a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile agents/weather_agent.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "import logging\n",
    "from typing import Optional\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from utils.logging import setup_logging\n",
    "\n",
    "# Initialize logger using the setup_logging function\n",
    "logger = setup_logging()\n",
    "\n",
    "# ----- Load environment variables -----\n",
    "load_dotenv(dotenv_path='env')\n",
    "\n",
    "# Global LLM instance\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# ----- Prompt Template -----\n",
    "city_extraction_template = \"\"\"\n",
    "Eres un asistente que extrae el nombre de la ciudad en inglés americano del siguiente texto. \n",
    "Responde solo con el nombre de la ciudad, sin comillas ni símbolos extra.\n",
    "\n",
    "Ejemplo:\n",
    "Texto: \"¿Cómo está el clima en Nueva York?\" -> New York\n",
    "Texto: \"{text}\"\n",
    "\"\"\"\n",
    "\n",
    "city_extraction_prompt = PromptTemplate(input_variables=[\"text\"], template=city_extraction_template)\n",
    "\n",
    "# ----- LLM-based city extractor -----\n",
    "def extract_city_with_llm(text: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extracts the name of the city from the given text using a language model.\n",
    "\n",
    "    Args:\n",
    "    - text (str): The input text that may contain a city name.\n",
    "\n",
    "    Returns:\n",
    "    - str or None: Returns the city name if successfully extracted, otherwise None.\n",
    "    \"\"\"\n",
    "    logger.debug(f\"Extracting city from text: '{text}'\")\n",
    "    prompt = city_extraction_prompt.format(text=text)\n",
    "\n",
    "    try:\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "        city = response.content.strip()\n",
    "        logger.info(f\"City extracted: '{city}'\")\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Error invoking the model for city extraction.\")\n",
    "        return None\n",
    "\n",
    "    if not city or len(city) < 2 or any(c in city for c in ['{', '}', '[', ']']):\n",
    "        logger.warning(f\"Invalid city detected: '{city}'\")\n",
    "        return None\n",
    "\n",
    "    return city\n",
    "\n",
    "# ----- Weather Node -----\n",
    "def get_weather(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Handles weather-related queries using an LLM to extract the city and the OpenWeatherMap API to fetch weather data.\n",
    "\n",
    "    Returns:\n",
    "    - 'results': If successful, a dictionary with the weather report.\n",
    "    - 'error': If an issue occurs, a dictionary with the error message.\n",
    "    - 'task_completed': A boolean flag indicating if the task was completed.\n",
    "    \"\"\"\n",
    "    # Añadir trazabilidad del nodo\n",
    "    state.setdefault(\"history\", []).append(\"task_weather\")\n",
    "\n",
    "    try:\n",
    "        input_text = state[\"messages\"][-1].content\n",
    "        logger.debug(f\"Received weather message: '{input_text}'\")\n",
    "\n",
    "        city = extract_city_with_llm(input_text)\n",
    "        logger.debug(f\"Respose llm: '{city}'\")\n",
    "\n",
    "        if not city:\n",
    "            msg = \"City could not be identified in the message.\"\n",
    "            logger.warning(msg)\n",
    "            return {\n",
    "                \"error\": {\"weather\": msg},\n",
    "                \"task_completed\": {\"weather\": False}\n",
    "            }\n",
    "\n",
    "        api_key = os.getenv(\"OPENWEATHER_API_KEY\")\n",
    "        if not api_key:\n",
    "            msg = \"API Key not configured in the system.\"\n",
    "            logger.error(msg)\n",
    "            return {\n",
    "                \"error\": {\"weather\": msg},\n",
    "                \"task_completed\": {\"weather\": False}\n",
    "            }\n",
    "\n",
    "        logger.info(f\"Fetching weather for: {city}\")\n",
    "        location_url = \"https://api.openweathermap.org/data/2.5/weather\"\n",
    "        params = {\n",
    "            \"q\": city,\n",
    "            \"appid\": api_key,\n",
    "            \"units\": \"metric\"\n",
    "        }\n",
    "        location_response = requests.get(location_url, params=params)\n",
    "\n",
    "        if location_response.status_code != 200:\n",
    "            msg = f\"City '{city}' not found or not correctly written in English.\"\n",
    "            logger.warning(msg)\n",
    "            return {\n",
    "                \"error\": {\"weather\": msg},\n",
    "                \"task_completed\": {\"weather\": False}\n",
    "            }\n",
    "\n",
    "        location_data = location_response.json()\n",
    "\n",
    "        try:\n",
    "            weather_desc = location_data[\"weather\"][0][\"description\"]\n",
    "            temperature = location_data[\"main\"][\"temp\"]\n",
    "        except (KeyError, IndexError, TypeError) as e:\n",
    "            msg = \"Unexpected weather data format received from API.\"\n",
    "            logger.exception(msg)\n",
    "            return {\n",
    "                \"error\": {\"weather\": msg},\n",
    "                \"task_completed\": {\"weather\": False}\n",
    "            }\n",
    "\n",
    "        weather_report = f\"The weather in {city} is {weather_desc} with a temperature of {temperature}°C.\"\n",
    "        logger.info(f\"Generated weather report: {weather_report}\")\n",
    "\n",
    "        return {\n",
    "            \"results\": {\"weather\": [weather_report]},\n",
    "            \"task_completed\": {\"weather\": True}\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        msg = f\"Error obtaining weather: {str(e)}\"\n",
    "        logger.exception(msg)\n",
    "        return {\n",
    "            \"error\": {\"weather\": msg},\n",
    "            \"task_completed\": {\"weather\": False}\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4da0fae6-5e64-4921-a019-cc54dccb9723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Ejemplo de prueba con distintos casos -----\n",
    "test_inputs = [\n",
    "    # Casos comunes\n",
    "    \"¿Cómo está el clima en Nueva York?\",  # Ciudad conocida\n",
    "    \"¿Cuál es el clima en Los Ángeles?\",  # Ciudad con nombre compuesto\n",
    "    \"Me gustaría saber cómo está el clima en Londres.\",  # Ciudad con nombre en inglés\n",
    "    \"¿Qué tal el clima en Ciudad de México?\",  # Ciudad con nombre compuesto en español\n",
    "\n",
    "    # Casos especiales\n",
    "    \"¿Qué tal está el clima?\",  # Sin mención de ciudad\n",
    "    \"El clima en 1234 podría cambiar.\",  # Texto con números, no ciudad\n",
    "    \"¿Cómo está el clima en Paris#@\",  # Ciudad con caracteres especiales\n",
    "    \"Hace calor en Barcelon\",  # Ciudad con nombre parcialmente correcto\n",
    "    \"¿Cómo está el clima en Shanghai?\",  # Ciudad conocida en otro idioma\n",
    "    \"¿En qué estado está el clima de Berlín?\",  # Pregunta con \"estado\", no ciudad\n",
    "    \"¿Cuánto falta para llegar a Tokyo? Me olvidé de la hora.\",  # Ciudad mencionada con información extra\n",
    "    \"¿Cómo está el clima en Saint Petersburg?\",  # Ciudad compuesta con un 'Saint'\n",
    "    \"¿Cómo está el clima en Amsterdam?\",  # Ciudad en inglés sin acento\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9c65ff1-91bd-4c12-9989-e022636edd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada: '¿Cómo está el clima en Nueva York?'\n",
      "Salida: {'results': {'weather': ['The weather in New York is clear sky with a temperature of 22.1°C.']}, 'task_completed': {'weather': True}}\n",
      "Entrada: '¿Cuál es el clima en Los Ángeles?'\n",
      "Salida: {'results': {'weather': ['The weather in Los Angeles is overcast clouds with a temperature of 15.93°C.']}, 'task_completed': {'weather': True}}\n",
      "Entrada: 'Me gustaría saber cómo está el clima en Londres.'\n",
      "Salida: {'results': {'weather': ['The weather in Londres is clear sky with a temperature of 9.96°C.']}, 'task_completed': {'weather': True}}\n",
      "Entrada: '¿Qué tal el clima en Ciudad de México?'\n",
      "Salida: {'results': {'weather': ['The weather in Mexico City is broken clouds with a temperature of 26.64°C.']}, 'task_completed': {'weather': True}}\n",
      "Entrada: '¿Qué tal está el clima?'\n",
      "Salida: {'error': {'weather': \"City 'No city mentioned' not found or not correctly written in English.\"}, 'task_completed': {'weather': False}}\n",
      "Entrada: 'El clima en 1234 podría cambiar.'\n",
      "Salida: {'results': {'weather': ['The weather in 1234 is scattered clouds with a temperature of 17.47°C.']}, 'task_completed': {'weather': True}}\n",
      "Entrada: '¿Cómo está el clima en Paris#@'\n",
      "Salida: {'results': {'weather': ['The weather in Paris is broken clouds with a temperature of 12.27°C.']}, 'task_completed': {'weather': True}}\n",
      "Entrada: 'Hace calor en Barcelon'\n",
      "Salida: {'results': {'weather': ['The weather in Barcelona is clear sky with a temperature of 16.7°C.']}, 'task_completed': {'weather': True}}\n",
      "Entrada: '¿Cómo está el clima en Shanghai?'\n",
      "Salida: {'results': {'weather': ['The weather in Shanghai is few clouds with a temperature of 16.92°C.']}, 'task_completed': {'weather': True}}\n",
      "Entrada: '¿En qué estado está el clima de Berlín?'\n",
      "Salida: {'results': {'weather': ['The weather in Berlin is clear sky with a temperature of 13.29°C.']}, 'task_completed': {'weather': True}}\n",
      "Entrada: '¿Cuánto falta para llegar a Tokyo? Me olvidé de la hora.'\n",
      "Salida: {'results': {'weather': ['The weather in Tokyo is mist with a temperature of 17.75°C.']}, 'task_completed': {'weather': True}}\n",
      "Entrada: '¿Cómo está el clima en Saint Petersburg?'\n",
      "Salida: {'results': {'weather': ['The weather in Saint Petersburg is overcast clouds with a temperature of 7.26°C.']}, 'task_completed': {'weather': True}}\n",
      "Entrada: '¿Cómo está el clima en Amsterdam?'\n",
      "Salida: {'results': {'weather': ['The weather in Amsterdam is few clouds with a temperature of 13.11°C.']}, 'task_completed': {'weather': True}}\n"
     ]
    }
   ],
   "source": [
    "# ----- Ejemplo de prueba con el ciclo -----\n",
    "for input_text in test_inputs:\n",
    "    state = {\n",
    "                \"messages\": [HumanMessage(content=input_text)],\n",
    "                \"tasks_to_do\": {},\n",
    "                \"results\": {},\n",
    "                \"error\": {},\n",
    "                \"order_task\": None,\n",
    "                \"ready_to_aggregate\": False,\n",
    "            }\n",
    "    # Llamada al nodo de clima\n",
    "    output = get_weather(state)\n",
    "    \n",
    "    print(f\"Entrada: '{input_text}'\")\n",
    "    print(f\"Salida: {output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a59913c-9886-41d5-8588-e2359279a350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': {'weather': ['The weather in Amsterdam is few clouds with a temperature of 13.11°C.']},\n",
       " 'task_completed': {'weather': True}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41276a78-020b-4d8c-9954-b5ffe679cc5c",
   "metadata": {},
   "source": [
    "\n",
    "### Consideraciones para el uso de LLMs para la extracción de ciudades\n",
    "\n",
    "Aunque los LLMs son bastante efectivos para extraer información de textos, hay ciertos aspectos que pueden influir en la precisión de la extracción, especialmente cuando se trata de errores tipográficos o casos especiales. A continuación se presentan algunos puntos clave a considerar:\n",
    "\n",
    "- **Errores tipográficos menores**: Los LLMs suelen manejar errores tipográficos leves (por ejemplo, \"Nwe York\" en lugar de \"New York\"), pero pueden fallar si los errores son más significativos (como \"Nuw York\" o \"New Yrok\").\n",
    "  \n",
    "- **Casos de ciudades homónimas**: En situaciones donde hay varias ciudades con el mismo nombre (como \"Paris\" en EE. UU. y \"París\" en Francia), un LLM puede no distinguir correctamente entre ellas, especialmente si el contexto es ambiguo.\n",
    "\n",
    "- **Contexto ambiguo**: Si el contexto en el texto no es claro o hay múltiples interpretaciones posibles (por ejemplo, \"La ciudad de México\"), el LLM podría extraer una ciudad incorrecta o no estar seguro de qué ciudad se menciona.\n",
    "\n",
    "- **Limitación en la precisión**: Aunque son robustos, los LLMs no siempre son infalibles para identificar la ciudad con precisión, especialmente cuando hay nombres de ciudades poco comunes o errores más complejos en el texto.\n",
    "\n",
    "#### Posible mejora:\n",
    "Se podría implementar un sistema **NERD (Reconocimiento de Entidades Nombradas)** especializado en ciudades para manejar estos casos con mayor precisión, ya que está diseñado para identificar entidades específicas de manera más confiable que los modelos de lenguaje general."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d366ae95-dbe9-4eba-baca-a36e405888ed",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Prototipado del Nodo \"Divisas\" y su Prompt\n",
    "\n",
    "En esta etapa se realiza el prototipado del nodo **get_exchange_rate**, cuyo objetivo es obtener la tasa de cambio entre el dólar estadounidense (USD) y el euro (EUR) usando la API de ExchangeRate. Además, se valida el diseño del **prompt** que guiará al modelo para realizar esta consulta correctamente.\n",
    "\n",
    "🎯 **Objetivo de la prueba**  \n",
    "Validar el funcionamiento del nodo **get_exchange_rate**, asegurando que:\n",
    "\n",
    "- El nodo realice la consulta correctamente a la API de ExchangeRate.\n",
    "- El manejo de errores sea adecuado en caso de problemas con la API o de respuesta inesperada.\n",
    "- El resultado tenga la estructura esperada, mostrando la tasa de cambio de USD a EUR.\n",
    "\n",
    "🧠 **Lógica del Nodo**  \n",
    "**get_exchange_rate** debe:\n",
    "\n",
    "1. Consultar la API de ExchangeRate con la clave de API configurada.\n",
    "2. Obtener la tasa de cambio entre USD y EUR.\n",
    "3. Devolver un diccionario con:\n",
    "   ```python\n",
    "   {\n",
    "       \"results\": [\"1 USD = X EUR\"],\n",
    "       \"error\": None o mensaje de error\n",
    "   }\n",
    "   ```\n",
    "   En caso de error (como problemas con la API), el campo \"error\" debe contener la descripción detallada, y \"results\" estará vacío.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac3008ba-a086-4d07-9c58-d67e0314d947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile agents/currency_agent.py\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import requests\n",
    "from typing import Optional\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ----- Configure logging -----\n",
    "from utils.logging import setup_logging\n",
    "\n",
    "# Initialize logger using the setup_logging function\n",
    "logger = setup_logging()\n",
    "\n",
    "# ----- Load environment variables -----\n",
    "# This loads environment variables from a .env file, typically containing sensitive information like API keys.\n",
    "load_dotenv(dotenv_path='env')\n",
    "\n",
    "# ----- Global LLM instance -----\n",
    "# Initializes the language model for currency extraction. \n",
    "# The model used here is GPT-3.5-turbo, which is ideal for text processing and extraction tasks.\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# ----- Prompt Template for currency extraction -----\n",
    "# This is the template used to instruct the language model to extract currency codes (ISO 4217 format) from the given text.\n",
    "# The prompt is written in Spanish but will be used to parse any input text in the same format.\n",
    "currency_extraction_template = \"\"\"\n",
    "Eres un asistente que extrae dos códigos de divisas (ISO 4217) desde el texto dado. \n",
    "Responde únicamente con los códigos separados por coma. No uses símbolos ni explicaciones.\n",
    "\n",
    "Ejemplo:\n",
    "Texto: \"¿Cuánto vale un dólar en pesos mexicanos?\" -> USD, MXN\n",
    "Texto: \"{text}\"\n",
    "\"\"\"\n",
    "\n",
    "currency_extraction_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],  # The input variable that will be passed to the template is 'text'.\n",
    "    template=currency_extraction_template  # The prompt template for extraction.\n",
    ")\n",
    "\n",
    "# ----- Function to extract currencies using the language model (LLM) -----\n",
    "def extract_currencies_with_llm(text: str) -> Optional[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Extracts two currency codes from a given text using the pre-defined language model prompt.\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): The input text containing the currencies to be extracted.\n",
    "    \n",
    "    Returns:\n",
    "    Optional[tuple[str, str]]: A tuple containing two ISO 4217 currency codes (or None if extraction fails).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Format the prompt with the provided text\n",
    "        prompt = currency_extraction_prompt.format(text=text)\n",
    "        logger.debug(f\"Prompt sent to LLM: {prompt}\")\n",
    "        \n",
    "        # Get the response from the LLM\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "        result = response.content.strip()\n",
    "        logger.debug(f\"LLM response: {result}\")\n",
    "\n",
    "        # Split the result into two parts (currency codes)\n",
    "        parts = [p.strip().upper() for p in result.split(\",\")]\n",
    "\n",
    "        # Check if the result contains exactly two 3-letter currency codes\n",
    "        if len(parts) == 2 and all(len(code) == 3 for code in parts):\n",
    "            logger.info(f\"Successfully extracted currency codes: {parts}\")\n",
    "            return parts[0], parts[1]\n",
    "        else:\n",
    "            logger.warning(f\"Unexpected format in the response: {result}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log the error if extraction fails\n",
    "        logger.exception(\"Error during currency extraction\")\n",
    "\n",
    "    return None\n",
    "\n",
    "# ----- Function to get exchange rate -----\n",
    "def get_exchange_rate(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Processes the user's message to detect currencies and fetches the exchange rate between them.\n",
    "    \n",
    "    Parameters:\n",
    "    state (dict): The current state of the agent, which contains the user's message and other context.\n",
    "    \n",
    "    Returns:\n",
    "    dict: The updated state dictionary containing either the results or error messages.\n",
    "    \"\"\"\n",
    "    # Añadir trazabilidad del nodo\n",
    "    state.setdefault(\"history\", []).append(\"task_exchange\")\n",
    "    try:\n",
    "        # Get the input text from the state (the user's message)\n",
    "        input_text = state[\"messages\"][-1].content\n",
    "        logger.info(f\"Processing user message: {input_text}\")\n",
    "\n",
    "        # Extract the currency codes from the user's input\n",
    "        currencies = extract_currencies_with_llm(input_text)\n",
    "        \n",
    "        # If no currencies are detected, return an error\n",
    "        if not currencies:\n",
    "            msg = \"No currencies detected in the message.\"\n",
    "            logger.warning(msg)\n",
    "            return {\n",
    "                \"error\": {\"exchange\": msg},\n",
    "                \"task_completed\":{\"exchange\": True} \n",
    "            }\n",
    "\n",
    "        # Extract base and target currencies\n",
    "        base_currency, target_currency = currencies\n",
    "        logger.info(f\"Detected currencies: {base_currency} -> {target_currency}\")\n",
    "\n",
    "        # Retrieve the API key for the exchange rate service from environment variables\n",
    "        api_key = os.getenv(\"EXCHANGE_API_KEY\")\n",
    "        if not api_key:\n",
    "            logger.error(\"API Key not set in the environment.\")\n",
    "            return {\n",
    "                \"error\": {\"exchange\": \"API Key not configured in the system.\"},\n",
    "                \"task_completed\":{\"exchange\": True} \n",
    "            }\n",
    "\n",
    "        # Construct the API URL to get the exchange rates\n",
    "        url = f\"https://v6.exchangerate-api.com/v6/{api_key}/latest/{base_currency}\"\n",
    "        logger.debug(f\"Querying external API: {url}\")\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the response from the API is successful\n",
    "        if response.status_code != 200:\n",
    "            logger.error(f\"Error in API response: {response.status_code}\")\n",
    "            return {\n",
    "                \"error\": {\"exchange\": f\"API error: {response.status_code}\"},\n",
    "                \"task_completed\":{\"exchange\": False} \n",
    "            }\n",
    "\n",
    "        # Parse the JSON data from the API response\n",
    "        data = response.json()\n",
    "\n",
    "        # Check if the target currency's exchange rate is available\n",
    "        if target_currency not in data.get('conversion_rates', {}):\n",
    "            logger.warning(f\"Exchange rate for {target_currency} not available in the response.\")\n",
    "            return {\n",
    "                \"error\": {\"exchange\": f\"Exchange rate for {target_currency} not found.\"},\n",
    "                \"task_completed\":{\"exchange\": True} \n",
    "            }\n",
    "\n",
    "        # Get the exchange rate for the target currency and format the response message\n",
    "        rate = data['conversion_rates'][target_currency]\n",
    "        message = f\"1 {base_currency} = {rate} {target_currency}\"\n",
    "        logger.info(f\"Exchange rate obtained: {message}\")\n",
    "\n",
    "        # Return the exchange rate result in the updated state\n",
    "        return {\n",
    "            \"results\": {\"exchange\": [message]},\n",
    "            \"task_completed\":{\"exchange\": True} \n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any unexpected errors and return them in the state\n",
    "        logger.exception(\"Unexpected error while getting exchange rate\")\n",
    "        return {\n",
    "            \"error\": {\"exchange\": f\"Error obtaining exchange rate: {str(e)}\"},\n",
    "            \"task_completed\":{\"exchange\": False} \n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "163c7629-14b1-4246-a894-82232c67f4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Entrada: 'Quiero saber la tasa de cambio de Estados Unidos y también el clima de Nueva York.'\n",
      "🧪 output: '{'results': {'exchange': ['1 USD = 19.6192 MXN']}, 'task_completed': {'exchange': True}}'\n",
      "🧪 Entrada: '¿Cuál es la tasa de cambio de euros y cómo está el clima en Londres?'\n",
      "🧪 output: '{'results': {'exchange': ['1 EUR = 0.8567 GBP']}, 'task_completed': {'exchange': True}}'\n",
      "🧪 Entrada: 'Me interesa saber cuánto está el dólar en euros y qué tiempo hace en París.'\n",
      "🧪 output: '{'results': {'exchange': ['1 USD = 0.8764 EUR']}, 'task_completed': {'exchange': True}}'\n",
      "🧪 Entrada: 'Quiero la tasa de cambio entre el dólar y el euro, además del clima en Tokio.'\n",
      "🧪 output: '{'results': {'exchange': ['1 USD = 0.8764 EUR']}, 'task_completed': {'exchange': True}}'\n",
      "🧪 Entrada: '¿Cómo está el clima en Madrid y qué tal la tasa de cambio del dólar?'\n",
      "🧪 output: '{'results': {'exchange': ['1 USD = 0.8764 EUR']}, 'task_completed': {'exchange': True}}'\n",
      "🧪 Entrada: 'Dime la tasa de cambio de Europa y el clima de Los Ángeles.'\n",
      "🧪 output: '{'results': {'exchange': ['1 EUR = 1.1411 USD']}, 'task_completed': {'exchange': True}}'\n",
      "🧪 Entrada: 'Quiero saber la tasa de cambio de Europa y cómo está el clima en Barcelona.'\n",
      "🧪 output: '{'results': {'exchange': ['1 EUR = 1.1411 USD']}, 'task_completed': {'exchange': True}}'\n",
      "🧪 Entrada: '¿Me dices la tasa de cambio del dólar a euro y también qué clima hace en Buenos Aires?'\n",
      "🧪 output: '{'results': {'exchange': ['1 USD = 0.8764 EUR']}, 'task_completed': {'exchange': True}}'\n",
      "🧪 Entrada: '¿Cuánto está el euro en dólares y cómo está el clima en Ciudad de México?'\n",
      "🧪 output: '{'results': {'exchange': ['1 EUR = 1.1411 USD']}, 'task_completed': {'exchange': True}}'\n",
      "🧪 Entrada: 'Estoy buscando la tasa de cambio entre el dólar y el euro, además del clima de Berlín.'\n",
      "🧪 output: '{'results': {'exchange': ['1 USD = 0.8764 EUR']}, 'task_completed': {'exchange': True}}'\n"
     ]
    }
   ],
   "source": [
    "# ----- Ejemplo de prueba con el ciclo -----\n",
    "test_inputs = [\n",
    "    \"Quiero saber la tasa de cambio de Estados Unidos y también el clima de Nueva York.\",\n",
    "    \"¿Cuál es la tasa de cambio de euros y cómo está el clima en Londres?\",\n",
    "    \"Me interesa saber cuánto está el dólar en euros y qué tiempo hace en París.\",\n",
    "    \"Quiero la tasa de cambio entre el dólar y el euro, además del clima en Tokio.\",\n",
    "    \"¿Cómo está el clima en Madrid y qué tal la tasa de cambio del dólar?\",\n",
    "    \"Dime la tasa de cambio de Europa y el clima de Los Ángeles.\",\n",
    "    \"Quiero saber la tasa de cambio de Europa y cómo está el clima en Barcelona.\",\n",
    "    \"¿Me dices la tasa de cambio del dólar a euro y también qué clima hace en Buenos Aires?\",\n",
    "    \"¿Cuánto está el euro en dólares y cómo está el clima en Ciudad de México?\",\n",
    "    \"Estoy buscando la tasa de cambio entre el dólar y el euro, además del clima de Berlín.\"\n",
    "]\n",
    "\n",
    "for input_text in test_inputs:\n",
    "    state = {\n",
    "                \"messages\": [HumanMessage(content=input_text)],\n",
    "                \"tasks_to_do\": {},\n",
    "                \"results\": {},\n",
    "                \"error\": {},\n",
    "                \"order_task\": None,\n",
    "                \"ready_to_aggregate\": False,\n",
    "            }\n",
    "\n",
    "    output = get_exchange_rate(state)\n",
    "\n",
    "    print(f\"🧪 Entrada: '{input_text}'\")\n",
    "    print(f\"🧪 output: '{output}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95a87e9-bd11-4e56-a3e5-96edcfcf5edf",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### Prototipado del Nodo \"Noticias\" y su Prompt\n",
    "\n",
    "En esta etapa se realiza el prototipado del nodo **get_news**, cuyo objetivo es obtener los titulares más importantes actuales de un país usando la API de NewsAPI. Se valida el diseño del **prompt** que guiará al modelo para realizar la consulta y mostrar los titulares.\n",
    "\n",
    "🎯 **Objetivo de la prueba**  \n",
    "Validar el funcionamiento del nodo **get_news**, asegurando que:\n",
    "\n",
    "- El nodo realice correctamente la consulta a la API de NewsAPI.\n",
    "- El manejo de errores sea adecuado en caso de problemas con la API o respuesta inesperada.\n",
    "- El resultado tenga la estructura esperada, mostrando los titulares de las noticias más recientes.\n",
    "\n",
    "🧠 **Lógica del Nodo**  \n",
    "**get_news** debe:\n",
    "\n",
    "1. Consultar la API de NewsAPI con la clave de API configurada.\n",
    "2. Obtener los titulares de noticias en algún pais.\n",
    "3. Devolver un diccionario con:\n",
    "   ```python\n",
    "   {\n",
    "       \"results\": [\"Titular 1, Titular 2, Titular 3\"],\n",
    "       \"error\": None o mensaje de error\n",
    "   }\n",
    "   ```\n",
    "   En caso de error (como problemas con la API), el campo \"error\" debe contener la descripción detallada, y \"results\" estará vacío."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2029154-8b40-47fc-b68f-d7b41a5d2d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile agents/news_agent.py\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ----- Configure logging -----\n",
    "from utils.logging import setup_logging\n",
    "\n",
    "# Initialize logger using the setup_logging function\n",
    "logger = setup_logging()\n",
    "\n",
    "# ----- Load environment variables -----\n",
    "# Loads environment variables from the .env file, typically containing API keys like the News API key.\n",
    "load_dotenv(dotenv_path='env')\n",
    "\n",
    "# ----- Global LLM instance -----\n",
    "# This initializes the LLM (language model) from OpenAI with the \"gpt-3.5-turbo\" model.\n",
    "# It's used for natural language understanding and extracting country codes from text.\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# ----- Prompt Template for country extraction -----\n",
    "# This is the prompt used by the LLM to extract the country code (ISO 3166-1 alpha-2) from the provided text.\n",
    "# The prompt asks for a 2-letter country code and returns only that code.\n",
    "country_extraction_template = \"\"\"\n",
    "You are an assistant that extracts the country (in ISO 3166-1 alpha-2 code, like 'MX', 'US', 'FR') from the following text.\n",
    "Respond only with the country code. If no country is mentioned, respond with ' '.\n",
    "\n",
    "Text: \"{text}\"\n",
    "\"\"\"\n",
    "\n",
    "# Create a prompt template that the LLM will use.\n",
    "country_extraction_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],  # The input variable for the template is 'text'.\n",
    "    template=country_extraction_template  # The actual template for extraction.\n",
    ")\n",
    "\n",
    "# ----- Function to extract country from the text using the LLM -----\n",
    "def extract_country_with_llm(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the country code from the provided text using the LLM.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text containing a country mention.\n",
    "\n",
    "    Returns:\n",
    "    str: The ISO 3166-1 alpha-2 country code extracted from the text (or ' ' if no country is mentioned).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Format the prompt with the provided text\n",
    "        prompt = country_extraction_prompt.format(text=text)\n",
    "        logger.debug(f\"Prompt sent to LLM: {prompt}\")\n",
    "        \n",
    "        # Get the response from the LLM\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "        country = response.content.strip().lower()  # Normalize the country code (convert to lowercase)\n",
    "        logger.debug(f\"LLM response: {country}\")\n",
    "\n",
    "        # Validate that the response is a valid 2-letter country code\n",
    "        if len(country) != 2 or not country.isalpha():\n",
    "            logger.warning(\"Invalid response from LLM\")\n",
    "            return None  # Return a blank space if the response is invalid\n",
    "        return country\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log any exception that occurs during country extraction\n",
    "        logger.exception(\"Error during country extraction\")\n",
    "        return None  # Return 'None' as a default fallback\n",
    "\n",
    "# ----- News fetching function -----\n",
    "def get_news(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Processes the input text to detect the country and fetches news headlines for that country.\n",
    "\n",
    "    Parameters:\n",
    "    input_text (str): The input text containing user query or message.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the news headlines or error information.\n",
    "    \"\"\"\n",
    "    # Añadir trazabilidad del nodo\n",
    "    state.setdefault(\"history\", []).append(\"task_news\")\n",
    "    \n",
    "    # Get the input text from the state (the user's message)\n",
    "    input_text = state[\"messages\"][-1].content\n",
    "    try:\n",
    "        logger.info(f\"Processing user message: {input_text}\")\n",
    "\n",
    "        # Extract the country code from the user's message\n",
    "        country_code = extract_country_with_llm(input_text)\n",
    "        logger.info(f\"Detected country code: {country_code}\")\n",
    "\n",
    "        # Retrieve the News API key from the environment variables\n",
    "        api_key = os.getenv(\"NEWS_API_KEY\")\n",
    "        if not api_key:\n",
    "            msg = \"News API Key is not configured.\"\n",
    "            logger.error(msg)\n",
    "            return {\n",
    "                    \"error\": {\"news\": msg},\n",
    "                   \"task_completed\": {\"news\": False}\n",
    "            }\n",
    "\n",
    "        # Construct the API URL to get news headlines for the detected country\n",
    "        url = f\"https://newsapi.org/v2/top-headlines?country={country_code}&apiKey={api_key}\"\n",
    "        logger.debug(f\"Querying News API: {url}\")\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Handle non-200 HTTP responses from the News API\n",
    "        if response.status_code != 200:\n",
    "            msg = f\"Error in News API: {response.status_code}\"\n",
    "            logger.error(msg)\n",
    "            return {\n",
    "                \"error\": {\"news\": msg},\n",
    "                \"task_completed\": {\"news\": False}\n",
    "            }\n",
    "\n",
    "        # Parse the JSON response from the News API\n",
    "        data = response.json()\n",
    "\n",
    "        # Check if the 'articles' key exists in the response and has data\n",
    "        if \"articles\" not in data or not data[\"articles\"]:\n",
    "            msg = f\"No news found for {country_code}.\"\n",
    "            logger.warning(msg)\n",
    "            return {\n",
    "                \"error\": {\"news\": msg},\n",
    "                \"task_completed\": {\"news\": False}\n",
    "                   }\n",
    "\n",
    "        # Extract the titles of the top 3 articles from the response\n",
    "        titles = \", \".join([article[\"title\"] for article in data[\"articles\"][:3]])\n",
    "        headlines = f\"Headlines in {country_code.upper()}: {titles}\"\n",
    "\n",
    "        logger.info(f\"Found headlines: {headlines}\")\n",
    "\n",
    "        # Return the news headlines as a dictionary\n",
    "        return {\"results\": {\"news\": [headlines]},\n",
    "                \"task_completed\": {\"news\": True}\n",
    "}\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any unexpected errors during the news retrieval process\n",
    "        logger.exception(\"Unexpected error while fetching news\")\n",
    "        return {\n",
    "            \"error\": {\"news\": f\"Error fetching news: {str(e)}\"},\n",
    "            \"task_completed\": {\"news\" : False}\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cb75e6-5d44-4337-972f-e852a44b2f7a",
   "metadata": {},
   "source": [
    "### Prototipado del Nodo  de gestion de errores y su Prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9875d113-76a7-412a-8dce-f16582cf4d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada: '¿Cuáles son las noticias hoy en UK?'\n",
      "Salida: '{'error': {'news': 'No news found for uk.'}, 'task_completed': {'news': False}}'\n",
      "Entrada: 'Dame los titulares más recientes de Estados Unidos.'\n",
      "Salida: '{'results': {'news': ['Headlines in US: Stock Market Today: Dow Jumps 800 Points; Gold Hits $3,500 on Tariff, Fed Worries — Live Updates - WSJ, Harvard sues the Trump administration in escalating confrontation - The Washington Post, Google Fi is launching a $35 / month unlimited plan - The Verge']}, 'task_completed': {'news': True}}'\n",
      "Entrada: 'Noticias de Irak por favor'\n",
      "Salida: '{'error': {'news': 'No news found for iq.'}, 'task_completed': {'news': False}}'\n",
      "Entrada: '¿Qué ha pasado últimamente en Argentina?'\n",
      "Salida: '{'error': {'news': 'No news found for ar.'}, 'task_completed': {'news': False}}'\n",
      "Entrada: '¿Qué está sonando en las noticias?'\n",
      "Salida: '{'error': {'news': 'No news found for None.'}, 'task_completed': {'news': False}}'\n",
      "Entrada: 'Me gustaría saber lo que pasa en Colombia y también cómo está el clima allá.'\n",
      "Salida: '{'error': {'news': 'No news found for co.'}, 'task_completed': {'news': False}}'\n"
     ]
    }
   ],
   "source": [
    "test_inputs = [\n",
    "    \"¿Cuáles son las noticias hoy en UK?\",\n",
    "    \"Dame los titulares más recientes de Estados Unidos.\",\n",
    "    \"Noticias de Irak por favor\",\n",
    "    \"¿Qué ha pasado últimamente en Argentina?\",\n",
    "    \"¿Qué está sonando en las noticias?\",\n",
    "    \"Me gustaría saber lo que pasa en Colombia y también cómo está el clima allá.\"\n",
    "]\n",
    "\n",
    "for input_text in test_inputs:\n",
    "    state = {\n",
    "                \"messages\": [HumanMessage(content=input_text)],\n",
    "                \"tasks_to_do\": {},\n",
    "                \"results\": {},\n",
    "                \"error\": {},\n",
    "                \"order_task\": None,\n",
    "                \"ready_to_aggregate\": False,\n",
    "            }\n",
    "\n",
    "    output = get_news(state)\n",
    "\n",
    "    print(f\"Entrada: '{input_text}'\")\n",
    "    print(f\"Salida: '{output}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b684dea-01b6-436f-9fb3-9b33e4821033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile nodes/error_handler.py\n",
    "\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from core.agent_state import AgentState  # Adjust if needed\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ----- Configurar logging -----\n",
    "from utils.logging import setup_logging\n",
    "\n",
    "# Initialize logger using the setup_logging function\n",
    "logger = setup_logging()\n",
    "\n",
    "# ----- Cargar variables de entorno -----\n",
    "load_dotenv(dotenv_path='env')\n",
    "\n",
    "# Global LLM instance\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# ----- Error Interpretation Prompt -----\n",
    "error_handler_template = PromptTemplate(\n",
    "    input_variables=[\"error\", \"original_text\"],\n",
    "    template=\"\"\"\n",
    "Eres un asistente experto en interpretar errores de sistemas que consultan datos sobre clima, noticias y divisas.\n",
    "\n",
    "Mensaje original del usuario:\n",
    "\"{original_text}\"\n",
    "\n",
    "Mensaje de error del sistema:\n",
    "\"{error}\"\n",
    "\n",
    "1. Si hay nombres de ciudades, países o monedas abreviados (como 'UK', 'US', 'EUR'), proporciónalos en su forma completa y clara.\n",
    "2. Genera una explicación amigable del error para el usuario.\n",
    "3. Sugiere una alternativa. Por ejemplo, si no se puede obtener el clima, sugiere obtener noticias o divisas, y viceversa.\n",
    "reiterando que vuleva a hacer la peticion con la recomenacion asociada\n",
    "Devuelve solo el texto final para el usuario, no incluyas explicaciones adicionales ni estructuras.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# ----- Error Handler Node -----\n",
    "def error_handler(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Uses LLM to transform technical error messages into user-friendly suggestions.\n",
    "\n",
    "    Parameters:\n",
    "        state (AgentState): The shared graph state including error and last message.\n",
    "\n",
    "    Returns:\n",
    "        dict: {\"results\": {...}, \"task_completed\": {...}, \"error\": {...}}\n",
    "    \"\"\"\n",
    "\n",
    "    # Añadir trazabilidad del nodo\n",
    "    state.setdefault(\"history\", []).append(\"task_error\")\n",
    "    try:\n",
    "        user_input = state[\"messages\"][-1].content if state.get(\"messages\") else \"\"\n",
    "\n",
    "        errores = state.get(\"error\", {})\n",
    "        nodo_source = next(iter(errores), \"desconocido\")  # Tomamos la primera clave\n",
    "        raw_error = errores.get(nodo_source, \"Error no especificado\")\n",
    "\n",
    "        logger.info(f\"Procesando error desde el nodo '{nodo_source}': {raw_error}\")\n",
    "\n",
    "        prompt = error_handler_template.format(\n",
    "            error=raw_error,\n",
    "            original_text=user_input\n",
    "        )\n",
    "        logger.debug(f\"Prompt generado para el LLM:\\n{prompt}\")\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "        friendly_message = response.content.strip()\n",
    "\n",
    "        # Remover la clave procesada\n",
    "        errores.pop(nodo_source, None)\n",
    "\n",
    "        logger.info(f\"Mensaje amigable generado: {friendly_message}\")\n",
    "        return {\n",
    "            \"results\": {nodo_source: [friendly_message]},\n",
    "            \"task_completed\": {nodo_source: True},\n",
    "            \"error\": { \"error\": errores } # Retornamos el dict sin la clave ya procesada\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Error en el manejador de errores\")\n",
    "        fallback_message = f\"No se pudo procesar el error automáticamente. Detalles: {str(e)}\"\n",
    "        return {\n",
    "            \"results\": {\"error\": [fallback_message]},\n",
    "            \"task_completed\": {'error': True}\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0d8d4e1-4bbb-43a9-8be1-542b5d12d238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weather': ['Lo siento, hubo un error al obtener el estado del clima en el Reino Unido. La API está fuera de servicio por el momento. Te recomendaría consultar las noticias o divisas en su lugar. Por favor, vuelve a hacer la petición con la recomendación asociada.']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "state = {\n",
    "            \"messages\": [HumanMessage(content=\"¿Qué clima hay en UK?\")],\n",
    "            \"tasks_to_do\": {},\n",
    "            \"results\": {},\n",
    "            \"error\": {\n",
    "            \"weather\" : \"Error al obtener el estado del clima:  La API esta fuera de servicio por el momento}\"\n",
    "            },\n",
    "            \"order_task\": None,\n",
    "            \"ready_to_aggregate\": False,\n",
    "        }\n",
    "\n",
    "print(error_handler(state)['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54f96d04-f6a8-4f39-ad0c-7d63be281269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='¿Qué clima hay en UK?', additional_kwargs={}, response_metadata={})],\n",
       " 'tasks_to_do': {},\n",
       " 'results': {},\n",
       " 'error': {},\n",
       " 'order_task': None,\n",
       " 'ready_to_aggregate': False,\n",
       " 'history': ['task_error']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b86662b-59a4-41a3-8e38-32e523bdd9d3",
   "metadata": {},
   "source": [
    "### Prototipado del Nodo order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18877895-6469-44d5-ad98-645c9ef93c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile nodes/order_tasks.py\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from core.agent_state import AgentState\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ----- Configurar logging -----\n",
    "from utils.logging import setup_logging\n",
    "\n",
    "# Initialize logger using the setup_logging function\n",
    "logger = setup_logging()\n",
    "\n",
    "# ----- Cargar variables de entorno -----\n",
    "load_dotenv(dotenv_path='env')\n",
    "\n",
    "# Global LLM instance\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# ----- Task Ordering Prompt -----\n",
    "order_tasks_template = PromptTemplate(\n",
    "    input_variables=[\"tasks\", \"user_input\"],\n",
    "    template=\"\"\"\n",
    "Eres un asistente experto en coordinar tareas de un sistema que puede obtener información sobre clima, noticias y divisas.\n",
    "\n",
    "El usuario ha solicitado información sobre las siguientes tareas: {tasks}.\n",
    "\n",
    "Consulta original del usuario:\n",
    "\"{user_input}\"\n",
    "\n",
    "Analiza la intención del usuario y determina el orden más lógico y útil en el que se deben ejecutar estas tareas. \n",
    "Devuelve únicamente un objeto JSON donde cada clave sea el nombre de la tarea y su valor sea su posición en el orden en el que aparece en el texto ,\n",
    "si no aparece alguna tarea omitela de la respuesta\n",
    "por ejemplo: {{\"weather\": 1, \"exchange\": 2, \"noticias\": 3}}\n",
    "\n",
    "No incluyas ningún otro texto ni explicaciones.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# ----- Task Ordering Node -----\n",
    "def order_tasks(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Orders tasks based on the user's input and their intent, helping to prioritize actions.\n",
    "\n",
    "    Parameters:\n",
    "        state (AgentState): The shared state that includes tasks and the user's query.\n",
    "\n",
    "    Returns:\n",
    "        dict: {\"results\": {\"order\": ...}, \"task_completed\": {\"order\": True}}\n",
    "    \"\"\"\n",
    "    # Añadir trazabilidad del nodo\n",
    "    state.setdefault(\"history\", []).append(\"task_order\")\n",
    "    tasks = {k: v for k, v in state.get(\"tasks\", {}).items() if v}\n",
    "    user_input = state[\"messages\"][-1].content if state.get(\"messages\") else \"\"\n",
    "\n",
    "    logger.info(f\"Tareas detectadas: {list(tasks.keys())}\")\n",
    "    logger.debug(f\"Consulta del usuario: {user_input}\")\n",
    "\n",
    "    try:\n",
    "        prompt = order_tasks_template.format(\n",
    "            tasks=\", \".join(tasks.keys()),\n",
    "            user_input=user_input\n",
    "        )\n",
    "        logger.debug(f\"Prompt generado para el LLM:\\n{prompt}\")\n",
    "\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "        ordered_dict = json.loads(response.content.strip())\n",
    "\n",
    "        logger.info(f\"Orden propuesto por LLM: {ordered_dict}\")\n",
    "\n",
    "        return {\n",
    "            \"order_task\":  ordered_dict,\n",
    "            \"task_completed\": {'order':True}\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error al ordenar tareas: {str(e)}\"\n",
    "        logger.exception(error_msg)\n",
    "        return {\n",
    "            \"task_completed\": {'order':False},\n",
    "            \"error\": {\"order\": error_msg}\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "466503f3-63cc-440e-ae4a-c1f0573f2035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.agent_state import AgentState  # Ajusta la importación según tu estructura\n",
    "\n",
    "# Estado de ejemplo\n",
    "state = {\n",
    "    \n",
    "    \"messages\": [HumanMessage(content=\"Quiero saber el clima y las noticias, pero en orden de importancia\")],\n",
    "    \"tasks\": {\n",
    "        \"weather\": True,  # Tarea activa\n",
    "        \"exchange\": True,  # Tarea activa\n",
    "        \"news\": True  # Tarea activa\n",
    "    },\n",
    "    \"error\": {},\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ede11519-f5b2-493f-845f-f9b005b049ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'order_task': {'weather': 1, 'news': 2}, 'task_completed': {'order': True}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Llama la función order_tasks\n",
    "new_state = order_tasks(state)\n",
    "\n",
    "# Imprime el nuevo estado para verificar la respuesta\n",
    "print(new_state)  # Aquí deberías ver el diccionario con el orden propuesto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8460b9-f09c-4a38-87d7-25b4a427432b",
   "metadata": {},
   "source": [
    "### Prototipado del Nodo integrador\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df3d6cdb-40ec-44d5-bfc6-1a9cf9d4f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile nodes/aggregator_tasks.py\n",
    "\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from core.agent_state import AgentState  # Ajustar según sea necesario\n",
    "\n",
    "# ----- Configurar logging -----\n",
    "from utils.logging import setup_logging\n",
    "\n",
    "# Initialize logger using the setup_logging function\n",
    "logger = setup_logging()\n",
    "\n",
    "# ----- Cargar variables de entorno -----\n",
    "load_dotenv(dotenv_path='env')\n",
    "\n",
    "# Instancia global de LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Plantilla para \"enchulamiento\"\n",
    "enchulador_template = PromptTemplate(\n",
    "    input_variables=[\"mensaje\"],\n",
    "    template=\"\"\"\n",
    "Enchula el siguiente mensaje para hacerlo amigable para el usuario:\n",
    "\n",
    "{mensaje}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "def aggregator(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Reformula resultados exitosos con el LLM y agrega errores directamente.\n",
    "    Devuelve mensajes listos para mostrar al usuario.\n",
    "\n",
    "    Args:\n",
    "        state (dict): Contiene 'order_task', 'results', 'error'.\n",
    "\n",
    "    Returns:\n",
    "        dict: {\"results\": {\"aggregator\": [messages]}, \"task_completed\":  {}}\n",
    "    \"\"\"\n",
    "    # Añadir trazabilidad del nodo\n",
    "    state.setdefault(\"history\", []).append(\"task_aggregator\")\n",
    "    processed_messages = []\n",
    "\n",
    "    logger.info(\"Iniciando agregación de tareas...\")\n",
    "\n",
    "    # Ordenar las tareas según el orden dado en el diccionario order_task\n",
    "    # Ordenamos el diccionario order_task por los valores (el orden de las tareas)\n",
    "    sorted_order = sorted(state.get(\"order_task\", {}).items(), key=lambda item: item[1])\n",
    "    logger.info(\"Orden detectado  %s\", sorted_order)\n",
    "\n",
    "    # Iterar en el orden correcto\n",
    "    for task, order in sorted_order:\n",
    "        result = state.get(\"results\", {}).get(task)\n",
    "        error = state.get(\"error\", {}).get(task)\n",
    "\n",
    "        logger.debug(f\"Tarea: {task} | Orden: {order} | Resultado: {result} | Error: {error}\")\n",
    "\n",
    "\n",
    "        if result:\n",
    "            mensaje_bruto = f\"{task.capitalize()}: {result}\"\n",
    "            try:\n",
    "                prompt_text = enchulador_template.format(mensaje=mensaje_bruto)\n",
    "                prompt = HumanMessage(content=prompt_text)\n",
    "                response = llm.invoke([prompt])\n",
    "                friendly_text = response.content.strip()\n",
    "                logger.info(f\"Mensaje procesado para '{task}': {friendly_text}\")\n",
    "                processed_messages.append(friendly_text)\n",
    "            except Exception as e:\n",
    "                logger.exception(f\"No se pudo reformular el mensaje para '{task}': {str(e)}\")\n",
    "                processed_messages.append(mensaje_bruto)\n",
    "\n",
    "    # Retornar solo el formato correcto, sin modificar el state\n",
    "    return {\n",
    "        \"results\": {\"aggregator\": processed_messages},\n",
    "        \"task_completed\": {'aggregator':True}\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49e13b39-ec27-45fc-9feb-2864c29317d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulación de estado con resultados y errores\n",
    "mock_state = {\n",
    "    \"order_task\": {  # Usamos 'order_task' con el orden de las tareas\n",
    "        \"weather\": 1,\n",
    "        \"exchange\": 2,\n",
    "        \"news\": 3\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"weather\": \"En CDMX hay 22°C y cielo parcialmente nublado.\",\n",
    "        \"exchange\": \"No se pudo obtener el tipo de cambio. Verifica la moneda solicitada.\",\n",
    "        \"news\": \"Elecciones en España dominan los titulares europeos.\"\n",
    "    },\n",
    "    \"error\": {\n",
    "\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Ejecutamos la función\n",
    "updated_state = aggregator(mock_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0eb4df70-d501-4d3f-8097-31f55cf0bcbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': {'aggregator': ['¡Hola! Te informo que en la Ciudad de México tenemos una temperatura de 22°C y el cielo se encuentra parcialmente nublado. ¡Que tengas un excelente día! 🌤️🌡️',\n",
       "   '¡Hola! Parece que no pudimos obtener el tipo de cambio para la moneda que solicitaste. Te recomendamos verificar la moneda que ingresaste. ¡Gracias por tu comprensión!',\n",
       "   '¡Hola! Te contamos que las elecciones en España están acaparando la atención de los titulares europeos. ¡No te pierdas las últimas noticias!']},\n",
       " 'task_completed': {'aggregator': True}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
